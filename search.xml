<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>宇宙第一模拟半导体元件的制造商TI？</title>
      <link href="/2023/02/27/%E5%85%AC%E5%8F%B8%E4%BB%8B%E7%BB%8D_TI/"/>
      <url>/2023/02/27/%E5%85%AC%E5%8F%B8%E4%BB%8B%E7%BB%8D_TI/</url>
      
        <content type="html"><![CDATA[<h1 id="德州仪器-Texas-Instruments"><a href="#德州仪器-Texas-Instruments" class="headerlink" title="德州仪器 Texas Instruments"></a><a href="https://www.ti.com.cn/">德州仪器 Texas Instruments</a></h1><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/TexasInstruments-Logo.svg/330px-TexasInstruments-Logo.svg.png">    </p><p>b站主页 <a href="https://space.bilibili.com/1444529151/video?tid=0&amp;page=4&amp;keyword=&amp;order=pubdate">https://space.bilibili.com/1444529151/video?tid=0&amp;page=4&amp;keyword=&amp;order=pubdate</a></p><blockquote><h1 id="以下内容来源于维基百科："><a href="#以下内容来源于维基百科：" class="headerlink" title="以下内容来源于维基百科："></a>以下内容来源于维基百科：</h1><p>德州仪器（英语：Texas Instruments），简称德仪（TI），是一家美国跨国科技公司，总部位于德克萨斯州的达拉斯[4]，以开发、制造、销售半导体和计算器技术闻名于世，主要从事数字信号处理与模拟电路方面的研究、制造和销售。它在25个国家有制造、设计或者销售机构。根据2021年IC Insights统计，德州仪器是世界第九大半导体制造商[5]；曾经是行动电话的第二大芯片供应商，仅次于高通；同时也是在世界范围内的第一大数字信号处理器（DSP）和模拟半导体元件的制造商[6]，其产品还包括计算器、微控制器以及多核处理器。德州仪器居世界半导体公司20强。德州仪器于1951年建立。它由地球物理业务公司（Geophysical Service Incorporated, GSI）整组而产生。这家公司最初生产地震工业和国防电子的相关设备。TI于1950年代初开始研究晶体管，同时也制造了世界上第一个商用硅晶体管。1954年，TI研发制造了第一台晶体管收音机，1958年，在TI中新研究实验室工作的Jack Kilby杰克·基尔比发明了集成电路。1961年，TI为美国空军制造了第一台集成电路电脑。50年代末期，TI开始研究红外线技术，随后TI涉足制造导弹和炸弹的雷达系统，导航和控制系统。世界上第一台便携式计算器由TI于1967年发明。20世纪70、80年代公司业务集中于家用电子产品，如数字钟表、电子手表、便携式计算器、家用电脑以及各种传感器。1997年公司将其国防业务出售给了雷神公司。2007年，德州仪器被认为是世界上最大的道德企业之一[7]。2011年收购美国国家半导体（National Semiconductor）之后，TI拥有由约45000种模拟电路产品及客户设计工具组成的投资组合[8]，这使TI成为世界上最大模拟电路元器件生产厂商。2011年TI在财富500强中位列第175名。TI旗下业务有两个主要分支：半导体（SC）和教育技术（ET），其中半导体业务创造了公司收益的约96%。</p></blockquote><blockquote><ul><li> 地球物理业务公司<br>德州仪器的历史可以追溯到1930年J·克莱伦斯·卡彻和尤金·麦克德莫特创建一个叫做地球物理业务公司的为石油工业提供地质探测的公司。在1939年，这个公司重组为Coronado公司。1941年12月6日，麦克德莫特和其他三名GSI的雇员J·埃里克·约翰逊、塞瑟尔·H·格林以及H·B·皮科克买下了GSI公司。在第二次世界大战期间，GSI为美国军用信号公司和美国海军制造电子设备。战争结束后，GSI公司继续其电子产品的生产。1951年，公司重新命名为德州仪器，GSI变为德州仪器的一个全资子公司。</li></ul></blockquote><blockquote><ul><li>国防领域<br>从1942年开始，德州仪器凭借潜水艇的探测设备开始进入国防电子领域。这些技术基于原来它为石油工业开发的地质探测技术。在20世纪80年代，这个产业的产品质量成为了新的焦点。80年代早期一个质量提升计划被启动。80年代晚期，德州仪器和伊士曼柯达公司和联合信号公司（Allied Signal）一起，开始参与摩托罗拉的六标准差规范的制定[10]。这类产品包括雷达系统、红外线系统、导弹、军用计算机、激光导航炸弹等。</li></ul></blockquote><blockquote><ul><li>半导体<br>早在1952年，德州仪器就从西部电子公司（Western Electric Co.，AT&amp;T的制造部门）以25,000美元的代价购买了生产晶体管的专利证书。到同年末，德州仪器已经开始制造和销售这些晶体管。公司副总裁帕特里克·哈格蒂颇有远见，意识到了电子技术领域的美好前景。随后，原本在新泽西州的贝尔实验室工作的戈登·K·蒂尔在看了一则纽约时报的广告后加入德州仪器，被哈格蒂任命为研究主任，回到了其故乡德克萨斯州工作。蒂尔在1953年1月将他在半导体晶体方面的专业知识带到了工作中。哈格蒂让他建立了一支由科学家和工程师组成的团队，使德州仪器保持半导体行业的领先地位。蒂尔的第一个任务是组织公司的中央研究实验室（Central Research Laboratories, CRL）。由于蒂尔的之前职业背景，这个新的部门基于贝尔实验室。另一名物理化学家，威尔克斯·阿道克斯，在1953年早些时候加入了德州仪器，开始领导一支较小的研究团队，致力于研制生长结晶体管。不久，阿道克斯成为了德州仪器的一名首席研究员[12]。</li></ul></blockquote><blockquote><ul><li>今天的TI<br>德州仪器的半导体产品几乎占了其收入的85%（2003年数据）[22]。在包括数字信号处理器、数字模拟转换器、模拟数字转换器、能源管理、模拟集成电路等不同产品领域都占据领先位置。无线通信也是德州仪器的一个焦点，目前全球有大约50%的移动电话都装有德州仪器生产的芯片。同时它也生产针对应用的集成电路以及单片机等。</li><li>无线终端商业单元数字光处理（DLP）</li><li>单片机</li><li>MSP430系列：低价、低功耗、用途广泛的嵌入式16位MCUTMS320系列：为实时控制应用进行优化的16/32位MCU家族<br>16位，整点运算，20至40兆赫<br>C28X：32位，整点或浮点运算，100至150兆赫数字信号处理器</li><li>TMS320系列<br>TMS320C2xxx：为控制应用优化的16和32位数字信号处理器<br>TMS320C5xxx：16位整点低功耗处理器，100至300兆赫<br>TMS320C6xxx：高性能数字信号处理器家族，300至1000赫兹<br>其他型号包括TMS320C33、TMS320C3x、TMS320C4x、TMS320C5x和TMS320C8x，以及为移动设备设计的基于ARM架构的多核处理器OMAP系列，如ARM9、ARM11和Cortex-A8等。</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 电路 </category>
          
          <category> 公司介绍 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 公司介绍 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跟着王小美和三岁一起学paddle(4)--卷积神经网络</title>
      <link href="/2023/02/26/%E8%B7%9F%E7%8E%8B%E5%B0%8F%E7%BE%8E%E4%B8%8E%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle%20%E2%80%94%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/02/26/%E8%B7%9F%E7%8E%8B%E5%B0%8F%E7%BE%8E%E4%B8%8E%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle%20%E2%80%94%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="跟王小美与三岁一起学paddle-—-卷积神经网络篇！"><a href="#跟王小美与三岁一起学paddle-—-卷积神经网络篇！" class="headerlink" title="跟王小美与三岁一起学paddle — 卷积神经网络篇！"></a>跟王小美与三岁一起学paddle — 卷积神经网络篇！</h1><p>简单有趣带你深度学习</p><p>跟王小美与三岁一起学paddle 第四讲</p><blockquote><p>致读者</p><p>在看这个notebook的你，对没错就是你</p><p>关注王小美喵，点个star⭐谢谢喵</p><p>注:本项目部分图片为自制，非授权请勿私自使用</p></blockquote><p><img src="https://ai-studio-static-online.cdn.bcebos.com/c40042ec75d5412da45370abe110559a5fc38c887cb24b2f9010409f4523d918"></p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>三岁老师，接下来是不是就要教我目标检测了呀？我已经准备好打穿csgo了 ٩(◕‿◕｡)۶ </p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>可以是可以，但是千万别把号玩封了啊，那我们就马上来打开新大门—-卷积神经网络</p><p>首先我们要从卷积学起来哦！(～o￣3￣)～</p><blockquote><h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><p>卷积运算是指从图像的左上角开始，开一个与模板同样大小的活动窗口，窗口图像与模板像元对应起来相乘再相加，并用计算结果代替窗口中心的像元亮度值。然后，活动窗口</p><p>向右移动一列，并作同样的运算。以此类推，从左到右、从上到下，即可得到一幅新图像。</p><p>我们也可以把卷积运算称之为滤波</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>为什么要对图片进行卷积运算呢？｡◕ᴗ◕｡</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>对于我们之前学的神经网络来说</p><p>一张图片所具有的信息量是非常大的，会有非常多的特征</p><p>假设我们用一台1200w像素的手机拍摄图片将他传入单隐藏层的神经网络就需要14G的显存，何况我们还要多层呢？</p><p>所以我们必须对他进行处理，提取重要信息</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/63aafb848ff74e3c9f86addf954feffb4b836520265741a28773a7537f3e7558"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><blockquote><p>对图像滤波要遵从两个原则</p><p>1.平移不变性</p><p>你找一个物品，不能受他位置的因素影响</p><p>2.局部性</p><p>你找一个物品，只需要在局部空间寻找</p><p>人们为了满足以上原则，使用了卷积</p></blockquote><p><img src="https://ai-studio-static-online.cdn.bcebos.com/b41d720b5f464120b0bb22c2a308daf2df9aa528999b4437b8f7b5d864dd376f"></p><blockquote><h1 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h1><p>将输入和卷积核进行交叉相关再加上偏置进行输出的层(对输入进行卷积运算的层)</p><p>让我们来手动进行一次卷积吧！░ ∗ ◕ ں ◕ ∗ ░(下面这个例子我的偏置为0) </p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/2e1225d8fc3c4d2e81071f933e6d6313030a7feb0e6940508aa37d37c09f4e8c"></p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/b5445b0cf2484d7fac0a1005a3d78119d782a92055ef4b6db2487d48bc1d0dc4"></p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>在上图中，卷积核(Kernel)类似于线性回归的权重(W)</p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>我对自己做了个卷积运算，大家快来看看吧！(o°ω°o)</p><p>不同的卷积核可以带来不同的效果哦，自己动手试试看吧</p><p><a href="https://github.com/Fafa-DL/Image-Augmentation">刚学习的小白可以使用啥都生大佬的图像增强工具</a></p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/2bdbd756f33e446db6769f53b20a03759329b19d1f6946acb28d8139f3ac34f3"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>假设原图像的高为H 宽为W</p><p>卷积核的高为Kh，宽为Kw</p><p>当我们对图像做卷积的时候，我们发现输出图像的高是(H-Kh+1)，宽是(W-Kw+1)</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/a18cd1427d154f65b7ed286369a20356a355a054ee9f49dc8fc225f370f8ee06"></p><p>我们会发现，通过卷积输出的图像是越来越小的，并且如果卷积核越大，输出的图像就越小，如果我们输入的图像比较小的时候这就不利于我们搭建多层网络了。</p><p>那么我们要如何对输入图像做处理呢？</p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>无敌的王小美出现了，并说了一句：把输入图像变大就行了。 o(&gt;ω&lt;)o</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><blockquote><h1 id="填充-padding"><a href="#填充-padding" class="headerlink" title="填充(padding)"></a>填充(padding)</h1><p>是的，在输入图像周围填充空白像素，这个操作我们叫做padding(填充)</p><p>假设我们填充Pw，Ph</p><p>那么输出图像的长是(W-Kw+Pw+1) 宽是(H-Kh+Ph+1)</p><p>当Pw = Kw-1， Ph = Kh-1 输出图像大小就与输入图像大小一样了</p><p> 当Kw是奇数时 Pw为偶数 我们会在上下左右padding P/2</p><p> 当Kw是偶数时，Pw为奇数，我们会padding 向上取整P/2(这个情况非常少见)</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>当我们输入的图像比较大，但是在深度学习中我们一般采用的卷积核是比较小的，我们就需要特别多的层才能把图像大小降下来，<br>大家知道层数越多计算就越复杂，我们不希望这样的情况发生</p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>这时候就要让卷积核迈大脚步了 o(&gt;ω&lt;)o</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p><img src="https://ai-studio-static-online.cdn.bcebos.com/943ec9d9834f4544985bdcb559df8038c65ea6ffa16f4138897b36ae741c3682"></p><blockquote><h1 id="步幅-Stride"><a href="#步幅-Stride" class="headerlink" title="步幅(Stride)"></a>步幅(Stride)</h1><p>卷积过程中，有时需要通过填充来避免信息损失，有时也要在卷积时通过设置的步长(Stride)来压缩一部分信息。因此卷积中的步幅是另一个构建卷积神经网络的基本操作。</p><p>例如下图</p></blockquote><p><img src="https://ai-studio-static-online.cdn.bcebos.com/dd096c7f528f4e7eb0a11f3a4ff3c15e89f45cfdcc5644f0993d8b3f63ec698e"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>这里补充一下通过卷积后输出的大小的计算公式</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/b1367802c1fb4229952cc4336cfd648be9e440e9f3d24bac8ad0c06659145e41"></p><h1 id="多通道的输入和输出"><a href="#多通道的输入和输出" class="headerlink" title="多通道的输入和输出"></a>多通道的输入和输出</h1><p>我们都知道图片是彩色的，一般来说图片有RGB三个通道</p><p>下面有请今天的玩伴女郎 — Lena同学！！！ ๑乛◡乛๑</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/968bd9ee4ed6486f943133dcf18f4b5360d6c859747c474eb65259d46bf939da"></p><p>为了直观显示，我把单个通道的亮度分别调到255</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/ccba15e7632f4a97ace26141c3a0b79ee43bc23e4bac48f985d9052edd7eed5d"></p><p>然后我们把三个通道的图片输入到网络中。不过我们要如何处理呢？</p><h1 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h1><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>我猜，刚刚一个输入用一个卷积核，现在有三个应该要用三个卷积核。(⊙ᗜ⊙)</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>没错，我们将每个通道的图片和一个卷积核卷积，三个通道就有三个卷积后的输出，然后我们将输出矩阵相加就可以得到一个输出。这样的方式我们可以叫他二维卷积</p><p>让我们来看下具体过程吧！(以输入通道为2为例)｡◕ᴗ◕｡</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/daf0b3cf82644e1d9a39f807a8de21500bbba30586c14b9bb8e62ecb92b9d7f3"></p><h1 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h1><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>我懂了，三个卷积核可以有一个输出，如果我要多通道输出的话，我整多批的三个卷积核对输入图像卷积我就可以得到多个输出了！ᕙ( * •̀ ᗜ •́ * )ᕗ</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>没错，这就是多输出的方法，我们也可以叫做三维卷积，可以看一下过程｡◕ᴗ◕｡</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/3917ee42758f4306b36006a8a38c48c98607a95e98b04c62bea2ce4390a84dc5"></p><p>我们可以将每个通道看作是对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>眼尖的同学是不是有一个疑问，为什么我上面图片中用到了一个1x1的卷积核</p><p>哈哈，这又是一个知识点了o(￣▽￣)ｄ</p><blockquote><h1 id="1×1-卷积层"><a href="#1×1-卷积层" class="headerlink" title="1×1 卷积层"></a>1×1 卷积层</h1><p>先说我们为什么要引入一个1x1的卷积层，我们发现，这个卷积层并不会压缩大小，而是将三个通道的信息进行了整合变成了一个通道</p><p>因为使用了最小窗口，卷积失去了在高度和宽度维度上识别相邻元素间相互作用的能力，而是在通道上具有了能力我们可以将1x1卷积层看作是在每个像素位置应用了全连接层</p><p>因此1x1卷积层是一个受欢迎的选择</p></blockquote><p><img src="https://ai-studio-static-online.cdn.bcebos.com/cd8e6e8cee2541debeb4e300a26135248d476eea8a714605b8ae7b1a7a7add1a"></p><blockquote><h1 id="第一阶段小结"><a href="#第一阶段小结" class="headerlink" title="第一阶段小结"></a>第一阶段小结</h1><p>1.卷积是对图像提取特征的操作</p><p>2.填充能减少边缘信息丢失</p><p>3.如果我们发现原始的输入分辨率十分冗余我们可以增大步幅</p><p>4.输出通道数是卷积层的超参数</p><p>5.二维卷积: 每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果</p><p>6.三维卷积: 每个输出通道有独立的三维卷积核</p></blockquote><blockquote><h1 id="汇聚层-池化层"><a href="#汇聚层-池化层" class="headerlink" title="汇聚层(池化层)"></a>汇聚层(池化层)</h1><p>由于卷积层对位置信息特别的敏感，可能因为物体位置不同导致卷积无法很好的处理。</p><p>这个时候就需要用到池化层</p><p>池化层能保持一定范围内的不变性，削弱卷积层对位置的过度敏感性。</p><p>池化有很多种  最大池化、平均池化、重叠池化、非重叠池化、金字塔池化SPP、双线性池化（Bilinear Pooling）</p><p>这里我主要讲最常用的最大池化和平均池化</p></blockquote><h1 id="最大池化-Max-Pooling"><a href="#最大池化-Max-Pooling" class="headerlink" title="最大池化(Max-Pooling)"></a>最大池化(Max-Pooling)</h1><p><img src="https://ai-studio-static-online.cdn.bcebos.com/d16e169b71b241d9b065cd61ac4ef3791f40b544bf824121a6baafdb0c5d5abb"></p><p>与卷积类似，最大池化也是以滑窗的方式进行，不过不同的是池化并没有用卷积核，而是算出跟窗口一样大的区域内的最大值。</p><p>池化和卷积一样也有填充和步幅</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>让我们来看看池化是如何削弱卷积对位置信息的过度敏感<p>对输入图像先用左边是1右边是-1的2x2卷积核进行卷积，再对输出结果做最大池化，我们会发现出现了一个像素的位移，使得卷积核对位置的敏感度降低</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/46556a0cb1ab4c3b8151d89cd8c00715e6dd283293de4c95af53aac33f9fe743"></p><h1 id="平均池化-Mean-Pooling"><a href="#平均池化-Mean-Pooling" class="headerlink" title="平均池化(Mean-Pooling)"></a>平均池化(Mean-Pooling)</h1><p>平均池化也是通过滑窗的方式，只不过将之前取最大的计算方式换成求平均</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/8cde5c74b9534ed083480131de93b3281f831d9568f2458ba5c21a2a81bb66e6"></p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>这两种不同的池化有啥区别吗ヾ（≧?≦）〃？？</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>一般来说，mean-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/3405a39e15fd41fda2d5a338c744cfe6f4ac16bf5f624015a30101c1de74bb82"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>好了，基本概念就讲到这了。让我们进入卷积神经网络吧！ o(&gt;ω&lt;)o</p><h1 id="经典卷积神经网络-LeNet-卷积神经网络中的HelloWorld"><a href="#经典卷积神经网络-LeNet-卷积神经网络中的HelloWorld" class="headerlink" title="经典卷积神经网络 LeNet(卷积神经网络中的HelloWorld)"></a>经典卷积神经网络 LeNet(卷积神经网络中的HelloWorld)</h1><p><img src="https://ai-studio-static-online.cdn.bcebos.com/7b987f92c45747fab679cf2ce5cc5d1fcac53a578c7e4a189e9263bf731d6c10"></p><blockquote><p>这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像LeCun.Bottou.Bengio.ea.1998中的手写数字。 当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。</p><p>当时，LeNet取得了与支持向量机（SVM）性能相媲美的成果，成为监督学习的主流方法。 LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。 时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>通过这样图我们可以对Lenet网络的结构有很好的认识。接下来让我们通过paddle来复现一下Lenet吧！o(&gt;ω&lt;)o</p><p>回顾一下我们之前讲的<a href="https://aistudio.baidu.com/aistudio/projectdetail/5378450?contributionType=1&sUid=2413201&shared=1&ts=1673573678148">softmax实现图像分类任务</a></p><p>为了能够应用softmax回归，我们首先将每个大小为28×28的图像使用paddle.nn.Flatten展平为一个784维的固定长度的一维向量，然后用全连接层对其进行处理。<br>这样的做法会丢失很多空间信息，而现在，我们已经掌握了卷积层的处理方法，我们可以在图像中保留空间结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn, optimizer</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2D(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    <span class="comment"># 一个输入6个输出 padding=2可以把28x28补到32x32，使用了sigmoid激活函数使其具有非线性性，</span></span><br><span class="line">    <span class="comment"># 关于激活函数的介绍我们下节课再讲，除了sigmoid你还可以是试试relu、tanh，relu是我们当今最常用的</span></span><br><span class="line">    nn.AvgPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2D(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    <span class="comment"># 由于后面要接全连接层所以这边将他摊平</span></span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于Lenet后面接的高斯层我们当今已经不再使用了，所以这里就将其去除了。除此之外该网络与上图一致</span></span><br></pre></td></tr></table></figure><pre><code>W0113 17:24:16.045086   180 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2W0113 17:24:16.049470   180 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让我们来看一下网络情况</span></span><br><span class="line">paddle.summary(net,(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br></pre></td></tr></table></figure><pre><code>--------------------------------------------------------------------------- Layer (type)       Input Shape          Output Shape         Param #    ===========================================================================   Conv2D-1       [[1, 1, 28, 28]]      [1, 6, 28, 28]          156         Sigmoid-1      [[1, 6, 28, 28]]      [1, 6, 28, 28]           0         AvgPool2D-1     [[1, 6, 28, 28]]      [1, 6, 14, 14]           0          Conv2D-2       [[1, 6, 14, 14]]     [1, 16, 10, 10]         2,416        Sigmoid-2     [[1, 16, 10, 10]]     [1, 16, 10, 10]           0         AvgPool2D-2    [[1, 16, 10, 10]]      [1, 16, 5, 5]            0          Flatten-1      [[1, 16, 5, 5]]          [1, 400]              0          Linear-1          [[1, 400]]            [1, 120]           48,120        Sigmoid-3         [[1, 120]]            [1, 120]              0          Linear-2          [[1, 120]]            [1, 84]            10,164        Sigmoid-4         [[1, 84]]             [1, 84]               0          Linear-3          [[1, 84]]             [1, 10]              850      ===========================================================================Total params: 61,706Trainable params: 61,706Non-trainable params: 0---------------------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.11Params size (MB): 0.24Estimated Total Size (MB): 0.35---------------------------------------------------------------------------&#123;&#39;total_params&#39;: 61706, &#39;trainable_params&#39;: 61706&#125;</code></pre><p>但是Sequential(循序容器)顾名思义，只能搭建循序执行的网络，一些跳跃连接的网络无法执行 ，所以可以用class创建一个类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Layer):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    继承paddle.nn.Layer定义网络结构</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化函数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2D(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),  <span class="comment"># 第一层卷积</span></span><br><span class="line">            nn.Sigmoid(), <span class="comment"># 激活函数</span></span><br><span class="line">            nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># 最大池化，下采样</span></span><br><span class="line">            nn.Conv2D(in_channels=<span class="number">6</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>), <span class="comment"># 第二层卷积</span></span><br><span class="line">            nn.Sigmoid(), <span class="comment"># 激活函数</span></span><br><span class="line">            nn.MaxPool2D(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>) <span class="comment"># 最大池化，下采样</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">400</span>, <span class="number">120</span>),  <span class="comment"># 全连接</span></span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),   <span class="comment"># 全连接</span></span><br><span class="line">            nn.Linear(<span class="number">84</span>, num_classes) <span class="comment"># 输出层</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向计算</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        y = self.features(inputs)</span><br><span class="line">        y = paddle.flatten(y, <span class="number">1</span>)</span><br><span class="line">        out = self.fc(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">network_2 = LeNet()</span><br><span class="line">paddle.summary(network_2,(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br></pre></td></tr></table></figure><pre><code>--------------------------------------------------------------------------- Layer (type)       Input Shape          Output Shape         Param #    ===========================================================================   Conv2D-3       [[1, 1, 28, 28]]      [1, 6, 28, 28]          60          Sigmoid-5      [[1, 6, 28, 28]]      [1, 6, 28, 28]           0         MaxPool2D-1     [[1, 6, 28, 28]]      [1, 6, 14, 14]           0          Conv2D-4       [[1, 6, 14, 14]]     [1, 16, 10, 10]         2,416        Sigmoid-6     [[1, 16, 10, 10]]     [1, 16, 10, 10]           0         MaxPool2D-2    [[1, 16, 10, 10]]      [1, 16, 5, 5]            0          Linear-4          [[1, 400]]            [1, 120]           48,120        Linear-5          [[1, 120]]            [1, 84]            10,164        Linear-6          [[1, 84]]             [1, 10]              850      ===========================================================================Total params: 61,610Trainable params: 61,610Non-trainable params: 0---------------------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.11Params size (MB): 0.24Estimated Total Size (MB): 0.35---------------------------------------------------------------------------&#123;&#39;total_params&#39;: 61610, &#39;trainable_params&#39;: 61610&#125;</code></pre><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>让我们来试试模型的效果吧！</p><p>还记得我们的步骤吗？</p><h1 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1.数据处理"></a>1.数据处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过paddle2.0的数据集读取api读取数据</span></span><br><span class="line"><span class="keyword">import</span> paddle.vision.transforms <span class="keyword">as</span> T</span><br><span class="line">transform = T.Normalize(mean=[<span class="number">127.5</span>], std=[<span class="number">127.5</span>]) <span class="comment"># 将数据归一化到[-1, 1]，参考原论文</span></span><br><span class="line">train_dataset = paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=transform)</span><br><span class="line">val_dataset =  paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=transform)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练集有&#123;&#125;\n测试集有&#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_dataset),<span class="built_in">len</span>(val_dataset)))</span><br></pre></td></tr></table></figure><pre><code>item    8/2421 [..............................] - ETA: 8s - 4ms/itemCache file /home/aistudio/.cache/paddle/dataset/mnist/train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz Begin to downloaditem 8/8 [============================&gt;.] - ETA: 0s - 5ms/itemDownload finishedCache file /home/aistudio/.cache/paddle/dataset/mnist/train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz Begin to downloadDownload finisheditem  12/403 [..............................] - ETA: 2s - 7ms/itemCache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-images-idx3-ubyte.gz Begin to downloaditem 2/2 [===========================&gt;..] - ETA: 0s - 3ms/itemDownload finishedCache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-labels-idx1-ubyte.gz Begin to downloadDownload finished训练集有60000测试集有10000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用matplotlib库绘制图像</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;图片：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(train_dataset[<span class="number">0</span>][<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(train_dataset[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Label：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(train_dataset[<span class="number">0</span>][<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(train_dataset[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化展示</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(train_dataset[<span class="number">0</span>][<span class="number">0</span>].reshape([<span class="number">28</span>,<span class="number">28</span>]), cmap=plt.cm.binary)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>图片：&lt;class &#39;numpy.ndarray&#39;&gt;(1, 28, 28)Label：&lt;class &#39;numpy.ndarray&#39;&gt;[5]/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/image.py:425: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead  a_min = np.asscalar(a_min.astype(scaled_dtype))/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/image.py:426: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead  a_max = np.asscalar(a_max.astype(scaled_dtype))</code></pre><p><img src="main_files/main_44_2.png" alt="png"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>还记得我们之前说的通过paddle api进行训练吗？ ٩(◕‿◕｡)۶</p><p>忘了的请务必再看一次，没点赞的话记得点个赞哈哈 </p><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/5378450?contributionType=1&sUid=2413201&shared=1&ts=1673582246540">不记得的话再看一次吧！点击跳转</a></p><h1 id="2-组网"><a href="#2-组网" class="headerlink" title="2.组网"></a>2.组网</h1><p>这里因为前面已经组好了所以直接跳下一步</p><h1 id="3-训练和调参"><a href="#3-训练和调参" class="headerlink" title="3.训练和调参"></a>3.训练和调参</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model = paddle.Model(network_2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型配置</span></span><br><span class="line">model.prepare(paddle.optimizer.Adam(learning_rate=<span class="number">0.001</span>, parameters=model.parameters()), <span class="comment"># Adam优化器</span></span><br><span class="line">              paddle.nn.CrossEntropyLoss(), <span class="comment"># 损失函数 交叉熵</span></span><br><span class="line">              paddle.metric.Accuracy()) <span class="comment"># 评估指标</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动全流程训练</span></span><br><span class="line">model.fit(train_dataset,  <span class="comment"># 训练数据集</span></span><br><span class="line">          val_dataset,   <span class="comment"># 评估数据集</span></span><br><span class="line">          epochs=<span class="number">5</span>,       <span class="comment"># 训练轮次</span></span><br><span class="line">          batch_size=<span class="number">64</span>,  <span class="comment"># 单次计算数据样本量</span></span><br><span class="line">          verbose=<span class="number">1</span>)      <span class="comment"># 日志展示形式，进度条</span></span><br></pre></td></tr></table></figure><pre><code>The loss value printed in the log is the current step, and the metric is the average value of previous steps.Epoch 1/5step 938/938 [==============================] - loss: 0.1858 - acc: 0.8464 - 10ms/step          Eval begin...step 157/157 [==============================] - loss: 0.0200 - acc: 0.9503 - 8ms/step          Eval samples: 10000Epoch 2/5step 938/938 [==============================] - loss: 0.0871 - acc: 0.9550 - 10ms/step          Eval begin...step 157/157 [==============================] - loss: 0.0068 - acc: 0.9703 - 8ms/step          Eval samples: 10000Epoch 3/5step 938/938 [==============================] - loss: 0.0128 - acc: 0.9667 - 10ms/step          Eval begin...step 157/157 [==============================] - loss: 0.0036 - acc: 0.9717 - 8ms/step          Eval samples: 10000Epoch 4/5step 938/938 [==============================] - loss: 0.0079 - acc: 0.9720 - 10ms/step          Eval begin...step 157/157 [==============================] - loss: 0.0026 - acc: 0.9774 - 8ms/step          Eval samples: 10000Epoch 5/5step 938/938 [==============================] - loss: 0.1188 - acc: 0.9763 - 9ms/step          Eval begin...step 157/157 [==============================] - loss: 4.6758e-04 - acc: 0.9826 - 8ms/step      Eval samples: 10000</code></pre><h1 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用测试集进行测试</span></span><br><span class="line">model.evaluate(val_dataset, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Eval begin...step 10000/10000 [==============================] - loss: 3.5763e-07 - acc: 0.9826 - 2ms/step          Eval samples: 10000&#123;&#39;loss&#39;: [3.576278e-07], &#39;acc&#39;: 0.9826&#125;</code></pre><blockquote><h1 id="Model-predict"><a href="#Model-predict" class="headerlink" title="Model.predict"></a>Model.predict</h1><p>高层 API 中提供了 Model.predict 接口，可对训练好的模型进行推理验证。只需传入待执行推理验证的样本数据，即可计算并返回推理结果。</p><p>返回格式是一个列表：</p><p>模型是单一输出：[(numpy_ndarray_1, numpy_ndarray_2, …, numpy_ndarray_n)]</p><p>模型是多输出：[(numpy_ndarray_1, numpy_ndarray_2, …, numpy_ndarray_n), (numpy_ndarray_1, numpy_ndarray_2, …, numpy_ndarray_n), …]</p><p>如果模型是单一输出，则输出的形状为 [1, n]，n 表示数据集的样本数。其中每个 numpy_ndarray_n 是对应原始数据经过模型计算后得到的预测结果，类型为 numpy 数组，例如 mnist 分类任务中，每个 numpy_ndarray_n 是长度为 10 的 numpy 数组。</p><p>如果模型是多输出，则输出的形状为[m, n]，m 表示标签的种类数，在多标签分类任务中，m 会根据标签的数目而定。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量预测测试集</span></span><br><span class="line">result = model.predict(val_dataset)</span><br></pre></td></tr></table></figure><pre><code>Predict begin...step 10000/10000 [==============================] - 2ms/step          Predict samples: 10000</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义画图方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_img</span>(<span class="params">img, predict</span>):</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(<span class="string">&#x27;predict: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(predict))</span><br><span class="line">    plt.imshow(img.reshape([<span class="number">28</span>, <span class="number">28</span>]), cmap=plt.cm.binary)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽样展示</span></span><br><span class="line">indexs = [<span class="number">2</span>, <span class="number">66</span>, <span class="number">33</span>, <span class="number">222</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> indexs:</span><br><span class="line">    show_img(val_dataset[idx][<span class="number">0</span>], np.argmax(result[<span class="number">0</span>][idx]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.argmax是用于取得数组中每一行或者每一列的的最大值</span></span><br></pre></td></tr></table></figure><p><img src="main_files/main_53_0.png" alt="png"></p><p><img src="main_files/main_53_1.png" alt="png"></p><p><img src="main_files/main_53_2.png" alt="png"></p><p><img src="main_files/main_53_3.png" alt="png"></p><h1 id="4-模型保存与加载"><a href="#4-模型保存与加载" class="headerlink" title="4.模型保存与加载"></a>4.模型保存与加载</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">model.save(<span class="string">&#x27;model/mnist&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载动态图模型参数和优化器参数</span></span><br><span class="line">model.load(<span class="string">&#x27;/home/aistudio/model/mnist&#x27;</span>)</span><br><span class="line">model.fit(train_dataset, epochs=<span class="number">2</span>, batch_size=<span class="number">64</span>, save_freq=<span class="number">1</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过它的参数 save_freq可以设置保存动态图模型的频率，即多少个 epoch 保存一次模型，默认值是 1。</span></span><br></pre></td></tr></table></figure><pre><code>The loss value printed in the log is the current step, and the metric is the average value of previous steps.Epoch 1/2step 938/938 [==============================] - loss: 0.0201 - acc: 0.9788 - 10ms/step          Epoch 2/2step 938/938 [==============================] - loss: 0.0183 - acc: 0.9808 - 9ms/step          </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用测试集进行测试</span></span><br><span class="line">model.evaluate(val_dataset, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Eval begin...step 10000/10000 [==============================] - loss: 5.9605e-07 - acc: 0.9840 - 2ms/step          Eval samples: 10000&#123;&#39;loss&#39;: [5.960463e-07], &#39;acc&#39;: 0.984&#125;</code></pre><h1 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h1><p>ID：Flose</p><p>School：浙大宁波理工学院</p><p>专业：自动化</p><p>宁理炼丹师协会-飞桨领航团QQ群：699816720</p><p>深度学习菜狗，正在不断努力，咱们一起加油</p>]]></content>
      
      
      <categories>
          
          <category> 计算机科学 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跟着王小美和三岁一起学paddle(3)--图像分类</title>
      <link href="/2023/02/26/%E8%B7%9F%E7%8E%8B%E5%B0%8F%E7%BE%8E%E4%B8%8E%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle%20%E2%80%94%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
      <url>/2023/02/26/%E8%B7%9F%E7%8E%8B%E5%B0%8F%E7%BE%8E%E4%B8%8E%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle%20%E2%80%94%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="跟王小美与三岁一起学paddle-—-图像分类！"><a href="#跟王小美与三岁一起学paddle-—-图像分类！" class="headerlink" title="跟王小美与三岁一起学paddle — 图像分类！"></a>跟王小美与三岁一起学paddle — 图像分类！</h1><hr><p>简单有趣带你图像分类</p><p>跟王小美与三岁一起学paddle 第三讲</p><blockquote><p>致读者</p><p>在看这个notebook的你，对没错就是你</p><p>关注王小美喵，点个star⭐谢谢喵    </p><p>注:本项目部分图片为自制，非授权请勿私自使用</p><p>下图是我做的一个小应用，后面有时间会出一期教大家如何从零到完成一个AI应用</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/603d410b1e394b1ea8e74af175a1d28d7bb1da96ce7b4f0a8f8ed19f6f16aacc"></p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>三岁老师，我打csgo又被打死了，您能不能教我AI自瞄啊！！ε(┬┬﹏┬┬)3、<p><img src="https://ai-studio-static-online.cdn.bcebos.com/b7a0185c1080494ebfc43babf31597640adc1c7ea6af4f99b2d2aee66abe893f"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>哎呀，怎么能开挂呢？不过AI自瞄倒是一个不错的技术，我们将他应用到生活中能给大家带来许多便利哦！(★ᴗ★)<p>要自瞄首先就要知道物体的位置，这时候就需要目标检测技术，在学习目标检测之前我们先从图像分类开始吧！！！</p><blockquote><h1 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h1><p>根据各自在图像信息中所反映的不同特征，把不同类别的目标区分开来的图像处理方法</p></blockquote><p>我们从一个图像分类问题开始。 假设每次输入是一个2x2的灰度图像。 我们可以用一个标量表示每个像素值。每个图像有四个特征x1，x2，x3，x4</p><p>假设每个图像属于类别“猫”“鸡”和“狗”中的一个。接下来，我们要选择如何表示标签？ヾ(=･ω･=)o</p><blockquote><h1 id="独热编码（one-hot-encoding）"><a href="#独热编码（one-hot-encoding）" class="headerlink" title="独热编码（one-hot encoding）"></a>独热编码（one-hot encoding）</h1><p>独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。</p><p>在我们的例子中，标签将是一个三维向量(1,0,0)代表“猫” ，(0,1,0)代表“鸡”，(0,0,1)代表“狗”</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>上节课我们将的线性回归是一个回归任务，而这节课的图像分类是个分类任务，让我们来看看他们的网络结构有什么不同吧！<p><img src="https://ai-studio-static-online.cdn.bcebos.com/4c7f4d56bbfe4df682df8025a6da10cac1bf148366a6406dbd2d9c9a26af99ec"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>像上图这样，每个输出都是由所有输入影响的网络层，我们称之为全连接层(Full connection layer)╮(￣▽￣)╭<p>上图中的O我们称之为 未规范化的预测（logit）</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>接下来就是本节课的重点了(敲黑板，哦不敲notebook)<p>相信你已经能知道我们是如何通过神经网络来预测分类结果了</p><blockquote><p>我们希望模型的输出Yj可以视为属于类j的概率， 然后选择具有最大输出值的类别作为我们的预测。<br>例如，如果Y1、Y2、Y3分别为0.1、0.8和0.1， 那么我们预测的类别是2，在我们的例子中代表“鸡”。</p></blockquote><p>但是我们通过上面的网络输出的Oj并不是像Yj这样和为1的正数，我们要如何对Oj进行处理呢？接下来就要用到softmax函数了</p><blockquote><h1 id="SoftMax函数"><a href="#SoftMax函数" class="headerlink" title="SoftMax函数"></a>SoftMax函数</h1><p>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 </p><p>为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。</p><p>为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/eeaf7777badd468a88e39bb64e6a6def4ae991d67ccc4a53aea8db15ec39d997"></p><p>softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率</p><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征决定。 因此，softmax回归是一个线性模型</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>那为什么不直接让 Oj/sum(Oj) 呢？<blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>其实这里面就涉及到许多数学上的内容了<p>其实我们使用softmax函数处理的话可以让我们输出的大部分值处在一个范围区间</p><p>在我们生活中，热水器的控温是不是没有在0到100度之间，而是在一个人体感受良好的范围内，这是不是就利于我们调节了</p><p>softmax函数其中就具有这个思想</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>得到预测值之后我们就需要设计损失函数来计算与真实值的损失从而调整权重了<blockquote><h1 id="交叉熵（cross-entropy-loss）"><a href="#交叉熵（cross-entropy-loss）" class="headerlink" title="交叉熵（cross-entropy loss）"></a>交叉熵（cross-entropy loss）</h1><p>交叉熵（Cross Entropy）是Shannon信息论中一个重要概念</p><p>它是分类问题最常用的损失之一</p><p>这边借用李沐大佬的图</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/e3c00c547f4341728f091e09c491a0dd073eafc78a664988b80a108339984a7f"></p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>那么交叉熵是怎么来的呢？<table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>竟然你诚心诚意的发问了，那我就大发慈悲的告诉你把!</p><p>要明白交叉熵首先要知道什么是熵</p><blockquote><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵也被称为熵，用来表示所有信息量的期望。</p><p>期望是试验中每次可能结果的概率乘以其结果的总和。</p><p>所以信息量的熵可表示为：</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/fcb825360a474ba68dcb2088a4bdb23037603101b6de4a36b02ee92279f5a763"></p><p>例如</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/48e30aab488543d68a955bea7381d3fc865c3f25e9834dd8a7e6335841553c7d"></p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>明白了熵的概念后，我们再来看一下什么是KL散度</p><blockquote><h1 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h1><p>如果对于同一个随机变量X有两个单独的概率分布P(x)和Q(x)，则我们可以使用KL散度来衡量这两个概率分布之间的差异。</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/32bf129008b243c7b85d6c644b31623c3e414380b0a641faa24b9bfcad9eaad1"></p><p>在我们的例子中，标签将是一个三维向量(1,0,0)代表“猫” ，(0,1,0)代表“鸡”，(0,0,1)代表“狗”</p><p>如果我们预测一张图像是猫的真实分布P(X)是(1,0,0) softmax函数输出是预测分布Q(X) = (0.7,0.2,0.1) </p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/d3ac37f8e6af429cac0eb61d013e3586717b0c9269df40fba83b1581d744be4f"></p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>将KL散度拆开，就能看到交叉熵了￣▽￣<p><img src="https://ai-studio-static-online.cdn.bcebos.com/ec8a5ca0eff44e21bea0d94e8fb63643144514cf4b6e4bc6b8163d4ef5b1e282"></p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/3cbc91b020254e279f3551619fd21aa48bc999edea514420a83fc423ba200968"></p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>原来是这样啊，我懂了！(p≧w≦q)</p><blockquote><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>softmax运算可以把网络输出结果转换成概率</p><p>softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。</p><p>交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>让我们来通过paddle的api快速对Mnist数据集实现softmax回归吧！ヾ(=･ω･=)o</p><p>Mnist数据集是一个手写数字数据集哦！</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/46ed8e93aeca4a059ed14cd72090cf4711f398e634c0400c8a14fef8f42967cd"></p><h1 id="SoftMax回归的代码实现-基于paddle2-0"><a href="#SoftMax回归的代码实现-基于paddle2-0" class="headerlink" title="SoftMax回归的代码实现(基于paddle2.0)"></a>SoftMax回归的代码实现(基于paddle2.0)</h1><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><blockquote><p>还记得我说的步骤吗？</p><p>1.数据处理</p><p>2.网络搭建</p><p>3.模型训练和调参</p><p>4.模型推理</p><p>5.模型部署</p></blockquote><h1 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1.数据处理"></a>1.数据处理</h1><h1 id="paddle数据集函数介绍"><a href="#paddle数据集函数介绍" class="headerlink" title="paddle数据集函数介绍"></a>paddle数据集函数介绍</h1><hr><blockquote><h1 id="paddle-vision-datasets-MNIST"><a href="#paddle-vision-datasets-MNIST" class="headerlink" title="paddle.vision.datasets.MNIST"></a>paddle.vision.datasets.MNIST</h1><p>MNIST 数据集的实现。</p><p>参数</p><p>image_path (str，可选) - 图像文件路径，如果 download 参数设置为 True，image_path 参数可以设置为 None。默认值为 None，默认存放在：~/.cache/paddle/dataset/mnist。</p><p>label_path (str，可选) - 标签文件路径，如果 download 参数设置为 True，label_path 参数可以设置为 None。默认值为 None，默认存放在：~/.cache/paddle/dataset/mnist。</p><p>mode (str，可选) - ‘train’ 或 ‘test’ 模式两者之一，默认值为 ‘train’。</p><p>transform (Callable，可选) - 图片数据的预处理，若为 None 即为不做预处理。默认值为 None。</p><p>download (bool，可选) - 当 data_file 是 None 时，该参数决定是否自动下载数据集文件。默认值为 True。</p><p>backend (str，可选) - 指定要返回的图像类型：PIL.Image 或 numpy.ndarray。必须是 {‘pil’，’cv2’} 中的值。如果未设置此选项，将从 paddle.vision.get_image_backend 获得这个值。默认值为 None。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">from</span> paddle <span class="keyword">import</span> nn <span class="comment"># 组网函数api</span></span><br><span class="line"><span class="comment"># 通过paddle2.0的数据集读取api读取数据</span></span><br><span class="line"><span class="keyword">from</span> paddle.vision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line">train_dataset = paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;train&#x27;</span>, transform=ToTensor())</span><br><span class="line">val_dataset =  paddle.vision.datasets.MNIST(mode=<span class="string">&#x27;test&#x27;</span>, transform=ToTensor())</span><br></pre></td></tr></table></figure><pre><code>item   26/2421 [..............................] - ETA: 5s - 2ms/itemCache file /home/aistudio/.cache/paddle/dataset/mnist/train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz Begin to downloaditem 8/8 [============================&gt;.] - ETA: 0s - 3ms/itemDownload finishedCache file /home/aistudio/.cache/paddle/dataset/mnist/train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz Begin to downloadDownload finisheditem  67/403 [===&gt;..........................] - ETA: 0s - 2ms/itemCache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-images-idx3-ubyte.gz Begin to downloaditem 2/2 [===========================&gt;..] - ETA: 0s - 2ms/itemDownload finishedCache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-labels-idx1-ubyte.gz Begin to downloadDownload finished</code></pre><h1 id="2-网络搭建"><a href="#2-网络搭建" class="headerlink" title="2.网络搭建"></a>2.网络搭建</h1><h1 id="组网函数介绍"><a href="#组网函数介绍" class="headerlink" title="组网函数介绍"></a>组网函数介绍</h1><blockquote><hr><h1 id="paddle-nn-Sequential"><a href="#paddle-nn-Sequential" class="headerlink" title="paddle.nn.Sequential"></a>paddle.nn.Sequential</h1><p>顺序容器。子 Layer 将按构造函数参数的顺序添加到此容器中。传递给构造函数的参数可以 Layers 或可迭代的 name Layer 元组。</p><p>softmax回归的输出层是一个全连接层。因此，为了实现我们的模型，我们只需在Sequential中添加一个带有10个输出的全连接层</p><hr><h1 id="paddle-nn-Flatten"><a href="#paddle-nn-Flatten" class="headerlink" title="paddle.nn.Flatten"></a>paddle.nn.Flatten</h1><p>构造一个 Flatten 类的可调用对象。它实现将一个连续维度的 Tensor 展平成一维 Tensor。</p><p>参数</p><p>start_axis (int，可选) - 展开的起始维度，默认值为 1。</p><p>stop_axis (int，可选) - 展开的结束维度，默认值为-1。</p><hr><h1 id="paddle-nn-ReLU"><a href="#paddle-nn-ReLU" class="headerlink" title="paddle.nn.ReLU"></a>paddle.nn.ReLU</h1><p>ReLU 激活层（Rectified Linear Unit）。计算公式如下：</p><p>ReLU(x)=max(0,x)</p><h1 id="paddle-nn-Dropout"><a href="#paddle-nn-Dropout" class="headerlink" title="paddle.nn.Dropout"></a>paddle.nn.Dropout</h1><p>Dropout 是一种正则化手段，该算子根据给定的丢弃概率 p，在训练过程中随机将一些神经元输出设置为 0，通过阻止神经元节点间的相关性来减少过拟合。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Net = paddle.nn.Sequential(</span><br><span class="line">    paddle.nn.Flatten(), <span class="comment"># 展平</span></span><br><span class="line">    paddle.nn.Linear(<span class="number">784</span>, <span class="number">512</span>),  <span class="comment"># 输入线性变换层数目为784个，输出为512个</span></span><br><span class="line">    paddle.nn.ReLU(),</span><br><span class="line">    paddle.nn.Dropout(<span class="number">0.2</span>),  <span class="comment"># 丢弃概率为0.2</span></span><br><span class="line">    paddle.nn.Linear(<span class="number">512</span>, <span class="number">10</span>)  <span class="comment"># 输入线性变换层数目为512个，输出为10个</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><blockquote><h1 id="3-模型训练"><a href="#3-模型训练" class="headerlink" title="3.模型训练"></a>3.模型训练</h1><p><a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/beginner/train_eval_predict_cn.html">关于使用paddle api进行模型训练的详细介绍</a></p><hr><h1 id="使用-paddle-Model-封装模型"><a href="#使用-paddle-Model-封装模型" class="headerlink" title="使用 paddle.Model 封装模型"></a>使用 paddle.Model 封装模型</h1><p>Model 对象是一个具备训练、测试、推理的神经网络。该对象同时支持静态图和动态图模式，通过 paddle.disable_static() 来切换。需要注意的是，该开关需要在实例化 Model 对象之前使用。输入需要使用 paddle.static.InputSpec 来定义。</p><p>使用高层 API 训练模型前，可使用 paddle.Model 将模型封装为一个实例，方便后续进行训练、评估与推理。</p><hr><h1 id="使用-Model-prepare-配置训练准备参数"><a href="#使用-Model-prepare-配置训练准备参数" class="headerlink" title="使用 Model.prepare 配置训练准备参数"></a>使用 Model.prepare 配置训练准备参数</h1><p>用 paddle.Model 完成模型的封装后，需通过 Model.prepare 进行训练前的配置准备工作，包括设置优化算法、Loss 计算方法、评价指标计算方法：</p><p>优化器（optimizer）：即寻找最优解的方法，可计算和更新梯度，并根据梯度更新模型参数。飞桨框架在 paddle.optimizer 下提供了优化器相关 API。并且需要为优化器设置合适的学习率，或者指定合适的学习率策略，飞桨框架在 paddle.optimizer.lr 下提供了学习率策略相关的 API。</p><p>损失函数（loss）：用于评估模型的预测值和真实值的差距，模型训练过程即取得尽可能小的 loss 的过程。飞桨框架在 paddle.nn Loss层 提供了适用不同深度学习任务的损失函数相关 API。</p><p>评价指标（metrics）：用于评估模型的好坏，不同的任务通常有不同的评价指标。飞桨框架在 paddle.metric 下提供了评价指标相关 API。</p><hr><h1 id="使用-Model-fit-训练模型"><a href="#使用-Model-fit-训练模型" class="headerlink" title="使用 Model.fit 训练模型"></a>使用 Model.fit 训练模型</h1><p>做好模型训练的前期准备工作后，调用 Model.fit 接口来启动训练。 训练过程采用二层循环嵌套方式：内层循环完成整个数据集的一次遍历，采用分批次方式；外层循环根据设置的训练轮次完成数据集的多次遍历。因此需要指定至少三个关键参数：训练数据集，训练轮次和每批次大小:</p><p>训练数据集：传入之前定义好的训练数据集。</p><p>训练轮次（epoch）：训练时遍历数据集的次数，即外循环轮次。</p><p>批次大小（batch_size）：内循环中每个批次的训练样本数。</p><p>除此之外，还可以设置样本乱序（shuffle）、丢弃不完整的批次样本（drop_last）、同步/异步读取数据（num_workers） 等参数，另外可通过 Callback 参数传入<br>回调函数，在模型训练的各个阶段进行一些自定义操作，比如收集训练过程中的一些数据和参数</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 预计模型结构生成模型实例，便于进行后续的配置、训练和验证</span></span><br><span class="line">model = paddle.Model(Net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练相关配置，准备损失计算方法，优化器和精度计算方法</span></span><br><span class="line">model.prepare(paddle.optimizer.Adam(parameters=Net.parameters()),</span><br><span class="line">              paddle.nn.CrossEntropyLoss(),</span><br><span class="line">              paddle.metric.Accuracy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adam 优化器出自 Adam 论文 的第二节，能够利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。</span></span><br><span class="line"><span class="comment">#CrossEntropyLoss 计算输入 input 和标签 label 间的交叉熵损失，它结合了 LogSoftmax 和 NLLLoss 的 OP 计算</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始模型训练</span></span><br><span class="line">model.fit(train_dataset,</span><br><span class="line">          epochs=<span class="number">5</span>,</span><br><span class="line">          batch_size=<span class="number">64</span>,</span><br><span class="line">          verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>The loss value printed in the log is the current step, and the metric is the average value of previous steps.Epoch 1/5step 938/938 [==============================] - loss: 0.1564 - acc: 0.9289 - 15ms/step          Epoch 2/5step 938/938 [==============================] - loss: 0.1323 - acc: 0.9687 - 16ms/step          Epoch 3/5step 938/938 [==============================] - loss: 0.0240 - acc: 0.9779 - 15ms/step          Epoch 4/5step 938/938 [==============================] - loss: 0.0049 - acc: 0.9826 - 16ms/step          Epoch 5/5step 938/938 [==============================] - loss: 0.0314 - acc: 0.9862 - 15ms/step          </code></pre><h1 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h1><p>通过loss和acc的结果来评断模型训练的质量和效率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(val_dataset, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># verbose (int，可选) - 可视化的模型，必须为 0，1，2。当设定为 0 时，</span></span><br><span class="line"><span class="comment"># 不打印日志，设定为 1 时，使用进度条的方式打印日志，设定为 2 时，一行一行地打印日志。默认值：2。</span></span><br></pre></td></tr></table></figure><pre><code>Eval begin...step 10000/10000 [==============================] - loss: 3.5763e-07 - acc: 0.9807 - 2ms/step          Eval samples: 10000&#123;&#39;loss&#39;: [3.5762793e-07], &#39;acc&#39;: 0.9807&#125;</code></pre><h1 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h1><p>模型训练后，训练好的模型参数保存在内存中，通常需要使用模型保存（save）功能将其持久化保存到磁盘文件中，并在后续需要训练调优或推理部署时，再加载（load）到内存中运行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;/home/aistudio/model/model&#x27;</span>)  <span class="comment"># 保存训练完的模型</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载动态图模型参数和优化器参数</span></span><br><span class="line">model.load(<span class="string">&#x27;/home/aistudio/model/model&#x27;</span>)</span><br><span class="line">model.fit(train_dataset, epochs=<span class="number">2</span>, batch_size=<span class="number">64</span>, save_freq=<span class="number">1</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过它的参数 save_freq可以设置保存动态图模型的频率，即多少个 epoch 保存一次模型，默认值是 1。</span></span><br></pre></td></tr></table></figure><pre><code>The loss value printed in the log is the current step, and the metric is the average value of previous steps.Epoch 1/2step 938/938 [==============================] - loss: 0.0352 - acc: 0.9883 - 15ms/step          Epoch 2/2step 938/938 [==============================] - loss: 0.0105 - acc: 0.9905 - 15ms/step          </code></pre><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>本次图像分类是介绍softmax回归的一个小任务，下次讲卷积神经网络的时候我会再和你一起完成一次哦！</p><p>今天就讲到这了，竟然没看睡着，不错嘛(o°ω°o)</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/72156cc5ac4a4708b387477e93b46320f186caf9868244e0a2349b38c33bd32d"></p><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>我懂了，原来图像分类是这样做的ヾ（≧?≦）〃<br>太棒了！！！</p><h1 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h1><p>ID：Flose</p><p>School：浙大宁波理工学院</p><p>专业：自动化</p><p>宁理炼丹师协会-飞桨领航团QQ群：699816720</p><p>深度学习菜狗，正在不断努力，咱们一起加油</p>]]></content>
      
      
      <categories>
          
          <category> 计算机科学 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跟着王小美和三岁一起学paddle--Tensor是什么？</title>
      <link href="/2023/02/26/%E8%B7%9F%E7%9D%80%E7%8E%8B%E5%B0%8F%E7%BE%8E%E5%92%8C%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle--Tensor%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
      <url>/2023/02/26/%E8%B7%9F%E7%9D%80%E7%8E%8B%E5%B0%8F%E7%BE%8E%E5%92%8C%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle--Tensor%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="跟王小美与三岁一起学paddle-—-开启新梦想"><a href="#跟王小美与三岁一起学paddle-—-开启新梦想" class="headerlink" title="跟王小美与三岁一起学paddle — 开启新梦想"></a>跟王小美与三岁一起学paddle — 开启新梦想</h1><p>你是否因为不懂基础知识而感到迷茫呢？</p><p>是否因为看不懂网上的视频而苦恼呢？</p><p>是否对paddle多种多样的api怎么使用而摆烂呢？</p><p>零基础？ 不用怕！我和你一样！</p><p>今天由小白新手王小美和三岁老师大家一起开启paddle学习新征程！</p><table><tr><td bgcolor=orange>王小美:</td></tr></table>大家都来认识一下我吧！٩(◕‿◕｡)۶<p><img src="https://ai-studio-static-online.cdn.bcebos.com/63bbf43fd807427bae56183ec3975566469464ba108f4401947ac6aa24743191"></p><blockquote><h1 id="人物设定"><a href="#人物设定" class="headerlink" title="[人物设定]"></a>[人物设定]</h1><p>姓名:王小美</p><p>年龄:13岁</p><p>身高:155</p><p>体重:null</p><p>角色性格:充满活力和好奇心的女生，她喜欢探索新的事物，并且对人工智能非常感兴趣。</p><p>她性格开朗，喜欢交朋友，并且乐于助人。虽然她有时会因为过于激动而鲁莽，但她的心思总是善良的。</p><p>所属社团:宁理炼丹师协会</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>通过三岁老师的<a href="https://aistudio.baidu.com/aistudio/projectdetail/1270135">本地paddle安装教程</a>我终于把paddle安装好了</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/4371210b8475477285f398b57d3c5dc94544e2acd42c4d27a6f54876010f0492"></p><table><tr><td bgcolor=orange>王小美:</td></tr></table>三岁老师，接下来是不是就可以用paddle来做出能帮我写论文的ai了？٩(◕‿◕｡)۶<table><tr><td bgcolor=baby blue>三岁:</td></tr></table><p>三岁:可以的哦，但是你得先学习基础知识哦，只要努力学习你一定能成为大牛的。先让我们从最基本的Tensor学起吧！ (★ᴗ★)</p><blockquote><h1 id="Tensor-gt-张量"><a href="#Tensor-gt-张量" class="headerlink" title="Tensor-&gt;张量"></a>Tensor-&gt;张量</h1><p>在paddle中和大多数的深度学习框架一样Tensor是运算的基础，那么什么是Tensor呢？让我们一起来看看吧！</p><p>这是paddle提供的一种数据结构和python的几种内置结构类型有所不同，他更类似于C语言的多维数组，和Numpy的array相类似</p><p>我们可以非常方便的读取到位置上的内容，但是不能够轻易的给已经生成的Tensor添加成员或者生成维度（优缺点）</p><p>所有的修改都需要通过新建在把数据处理后复制进去（paddle对此作了一定程度的封装，便于使用）</p><p>所谓千言万语不如一张图的信息量大这里我用三岁大佬的一张图来进行概括</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/9ae3002c51794d17b9003e8337e3d4dc244339c89e524c01bb64176df1d0659a"></p><p>眼尖的同学可能已经发现了，王小美同学在检验paddle是否成功安装的时候使用paddle的api生成了一个Tensor并将他打印出来了，让我们用notebook来复现一下吧！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先我们导入paddle包</span></span><br><span class="line"><span class="keyword">import</span> paddle </span><br><span class="line">tensor_01 = paddle.to_tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tensor_01</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[3], dtype=int64, place=Place(cpu), stop_gradient=True,       [1, 2, 3])</code></pre><table><tr><td bgcolor=orange>王小美:</td></tr></table>哇输出了好多东西，看英文我都看得懂但是合起来就不懂是啥了ε(┬┬﹏┬┬)3<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>没关系，我来为你解释一下吧！<blockquote><p>结果解析</p><p>shape=[3]：一维长度为3；</p><p>dtype=float64：类型是64位的；</p><p>place=Place(gpu:0) 使用的是gpu<br>如果这里是CPUPlace：使用的是cpu；</p><p>stop_gradient=True：不求导，不参加梯度更新；</p><p>[1., 2., 3.]内容是[1., 2., 3.]</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>原来是这样啊，这个tensor我会了，看我操作吧！<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成一个只有单个整数1的tensor</span></span><br><span class="line">int_1 = paddle.to_tensor([<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 生成一个带有浮点的只有单个浮点数3.14的tensor</span></span><br><span class="line">float_1 = paddle.to_tensor([<span class="number">3.14</span>])</span><br><span class="line"><span class="comment"># 生成二维tensor</span></span><br><span class="line">dim2_tensor = paddle.to_tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>],</span><br><span class="line">                                  [<span class="number">4.0</span>,<span class="number">5.0</span>,<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成三维tensor</span></span><br><span class="line">dim3_tensor = paddle.to_tensor([[[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>],</span><br><span class="line">                                [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>],</span><br><span class="line">                                [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;只有单个整数1的tensor\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(int_1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;二维的tensor\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(dim2_tensor))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;三维的tensor\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(dim3_tensor))</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>只有单个整数1的tensorTensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=True,       [1])二维的tensorTensor(shape=[2, 3], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[1., 2., 3.],        [4., 5., 6.]])三维的tensorTensor(shape=[1, 3, 3], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[[1., 2., 3.],         [1., 2., 3.],         [1., 2., 3.]]])</code></pre><table><tr><td bgcolor=orange>王小美:</td></tr></table>三岁老师，我还发现了一个问题，通过观察最外层的"["数量我就可以判断这个tensor是几维的了  ￣︶￣）↗<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>不错哦！看来你已经学会了使用paddle的api来生成tensor了   d=====(￣▽￣*)b<table><tr><td bgcolor=orange>王小美:</td></tr></table>对了三岁老师，我觉得这个tensor和我平时使用的numpy里面的array好像啊，他们可以互相替换吗？(*╯3╰)<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>哈哈，看来你已经发现了啊，其实使用paddle的api我们就可以实现两者之间的转换了哦！(★＞U＜★)<blockquote><h1 id="Tensor与Numpy-array的相互转换"><a href="#Tensor与Numpy-array的相互转换" class="headerlink" title="Tensor与Numpy array的相互转换"></a>Tensor与Numpy array的相互转换</h1><p>由于Tensor与Numpy array在表现上极为相似，转换也便存在可能</p><p>使用Tensor.numpy()即可轻松装换由Tensor转换成Numpy</p><p>使用paddle.to_tensor(Numpy array(xxx))可以把Numpy转换成Tensor</p><p>创建的 Tensor 与原 Numpy array 具有相同的 shape 与 dtype。</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>明白了，看我操作(★ᴗ★)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入numpy</span></span><br><span class="line"><span class="keyword">import</span> numpy</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#dim3_tensor转换成numpy.array</span></span><br><span class="line">dim3_tensor.numpy()</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)/tmp/ipykernel_211/822791399.py in &lt;module&gt;      1 #dim3_tensor转换成numpy.array----&gt; 2 dim3_tensor.numpy()NameError: name &#39;dim3_tensor&#39; is not defined</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Numpy转换成Tensor</span></span><br><span class="line">numpy_to_tensor = paddle.to_tensor(numpy.array([<span class="number">1.0</span>, <span class="number">2.0</span>]))</span><br><span class="line">numpy_to_tensor</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2], dtype=float64, place=Place(gpu:0), stop_gradient=True,       [1., 2.])</code></pre><table><tr><td bgcolor=orange>王小美:</td></tr></table>wow 成功了！ paddle真是太方便了<blockquote><h1 id="Tensor报错"><a href="#Tensor报错" class="headerlink" title="Tensor报错"></a>Tensor报错</h1><p>Tensor只支持规则的矩阵，对于非规则的会抛出异常！</p><p>也就是同一个维度上大小、类型要相同！</p></blockquote><p>平行宇宙的王小美<table><tr><td bgcolor=orange>王小美:</td></tr></table><br>啊呀，三岁老师我的代码报错了，我看不懂你能帮我看看吗?</p><h1 id="维数不对会报错！"><a href="#维数不对会报错！" class="headerlink" title="维数不对会报错！"></a>维数不对会报错！</h1><p>例如下列代码</p><p>rank_2_tensor = paddle.to_tensor([[1.0, 2.0],<br>                                  [4.0, 5.0, 6.0]])</p><h1 id="输出结果"><a href="#输出结果" class="headerlink" title="输出结果"></a>输出结果</h1><p>/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:345: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify ‘dtype=object’ when creating the ndarray<br>  data = np.array(data)</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>昨天有个同学跑过来问我，老师一个三行四列的行列式怎么算？你这有异曲同工之妙啊<p><img src="https://ai-studio-static-online.cdn.bcebos.com/0f69a0a5bed14c1b80aa54535c7df6fd63798e222e7b43309a5450737fef2c32"></p><blockquote><h1 id="创建一个指定shape的Tensor"><a href="#创建一个指定shape的Tensor" class="headerlink" title="创建一个指定shape的Tensor"></a>创建一个指定shape的Tensor</h1><p>Paddle提供了一些API</p><p>paddle.zeros([m, n])                # 创建数据全为0，shape为[m, n]的Tensor</p><p>paddle.ones([m, n])                 # 创建数据全为1，shape为[m, n]的Tensor</p><p>paddle.full([m, n], 10)             # 创建数据全为10，shape为[m, n]的Tensor</p><p>paddle.arrange(start, end, step)    # 创建从start到end，步长为step的Tensor</p><p>paddle.linspace(start, end, num)    # 创建从start到end，元素个数固定为num的Tensor</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>太棒了！这样就能快速生成tensor了  (～￣▽￣)～<blockquote><h1 id="Tensor的shape（形状）"><a href="#Tensor的shape（形状）" class="headerlink" title="Tensor的shape（形状）"></a>Tensor的shape（形状）</h1><table><thead><tr><th>名称</th><th>属性</th></tr></thead><tbody><tr><td>shape</td><td>tensor的每个维度上的元素数量</td></tr><tr><td>rank</td><td>tensor的维度的数量，例如vector的rank为1，matrix的rank为2.</td></tr><tr><td>axis/dimension</td><td>tensor某个特定的维度</td></tr><tr><td>size</td><td>tensor中全部元素的个数</td></tr></tbody></table></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>你光说我也看不懂啊！<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>dont worry it if you dont understand我来举个例子<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rank_4_tensor = paddle.ones([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(rank_4_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data Type of every element:&quot;</span>, rank_4_tensor.dtype)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of dimensions:&quot;</span>, rank_4_tensor.ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape of tensor:&quot;</span>, rank_4_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Elements number along axis 0 of tensor:&quot;</span>, rank_4_tensor.shape[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Elements number along the last axis of tensor:&quot;</span>, rank_4_tensor.shape[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[2, 3, 4, 5], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[[[1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.]],         [[1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.]],         [[1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.]]],        [[[1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.]],         [[1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.]],         [[1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.],          [1., 1., 1., 1., 1.]]]])Data Type of every element: paddle.float32Number of dimensions: 4Shape of tensor: [2, 3, 4, 5]Elements number along axis 0 of tensor: 2Elements number along the last axis of tensor: 5W0107 17:10:06.180053   179 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2W0107 17:10:06.184638   179 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.</code></pre><blockquote><h3 id="解析结果"><a href="#解析结果" class="headerlink" title="解析结果"></a>解析结果</h3><p>Data Type of every element: VarType.FP32</p><p>数据类型是32位的</p><p>Number of dimensions: 4</p><p>维度是4维</p><p>Shape of tensor: [2, 3, 4, 5]</p><p>大小是[2, 3, 4, 5]</p><p>Elements number along axis 0 of tensor: 2</p><p>0维度的数量</p><p>Elements number along the last axis of tensor: 5</p><p>最后一个维度的数量</p><p>图解：<br><img src="https://ai-studio-static-online.cdn.bcebos.com/6850ba53a3f544ec894510bd87a0ce703067b1da7d984abfb7e144c0e68dcf4b"></p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>太棒了，我完全会了，爱了<h1 id="改变维度"><a href="#改变维度" class="headerlink" title="改变维度"></a>改变维度</h1><table><tr><td bgcolor=orange>王小美:</td></tr></table>对了 三岁老师，我知道numpy可以reshape，在paddle里面也可以直接reshape吗？ ヾ（≧?≦）〃<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>是的，你来试试吧！ (★ᴗ★)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor_x = paddle.arange(<span class="number">0</span>, <span class="number">4</span>, <span class="number">1</span>, paddle.float64)</span><br><span class="line">tensor_x_reshape = paddle.reshape(tensor_x, [<span class="number">4</span>,<span class="number">1</span>])</span><br><span class="line">tensor_x_reshape </span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[4, 1], dtype=float64, place=Place(gpu:0), stop_gradient=True,       [[0.],        [1.],        [2.],        [3.]])</code></pre><table><tr><td bgcolor=orange>王小美:</td></tr></table>wow 成功了<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>1. -1 表示这个维度的值是从Tensor的元素总数和剩余维度推断出来的。因此，有且只有一个维度可以被设置为-1。<ol start="2"><li>0 表示实际的维数是从Tensor的对应维数中复制出来的，因此shape中0的索引值不能超过x的维度。</li></ol><blockquote><h1 id="paddle-Tensor的dtype"><a href="#paddle-Tensor的dtype" class="headerlink" title="paddle Tensor的dtype"></a>paddle Tensor的dtype</h1><p>Tensor的数据类型，可以通过 Tensor.dtype 来查看，dtype支持：’bool’，’float16’，’float32’，’float64’，’uint8’，’int8’，’int16’，’int32’，’int64’。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看tensor_x的类型</span></span><br><span class="line">tensor_x.dtype</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------NameError                                 Traceback (most recent call last)/tmp/ipykernel_211/2807053611.py in &lt;module&gt;      1 # 查看tensor_x的类型----&gt; 2 tensor_x.dtypeNameError: name &#39;tensor_x&#39; is not defined</code></pre><blockquote><h1 id="dtype类型的指定"><a href="#dtype类型的指定" class="headerlink" title="dtype类型的指定"></a>dtype类型的指定</h1><p>通过Python元素创建的Tensor，可以通过dtype来进行指定，如果未指定：</p><p>对于python整型数据，则会创建int64型Tensor</p><p>对于python浮点型数据，默认会创建float32型Tensor</p><p>如果对浮点型默认的类型进行修改可以使用set_default_type进行调整</p><p>通过Numpy array创建的Tensor，则与其原来的dtype保持相同</p></blockquote><p>在创建tensor的时候我们就可以通过在后面加类型的方法来指定tensor类型</p><p>我们可以看到，王小美同学在<strong>改变维度</strong>中使用了</p><p>tensor_x = paddle.arange(0, 4, 1, paddle.float64)<br>这样就创建了一个0到4类型为float64的Tensor</p><blockquote><h1 id="Tensor的place"><a href="#Tensor的place" class="headerlink" title="Tensor的place"></a>Tensor的place</h1><p>初始化Tensor时可以通过place来指定其分配的设备位置，可支持的设备位置有三种：CPU/GPU/固定内存</p><p>其中固定内存也称为不可分页内存或锁页内存，其与GPU之间具有更高的读写效率，并且支持异步传输，这对网络整体性能会有进一步提升，但其缺点是分配空间过多时可能会降低主机系统的性能，因为其减少了用于存储虚拟内存数据的可分页内存。</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>通过以下代码就能将tensor放在不同的地方哦！<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建CPU上的Tensor</span></span><br><span class="line">cpu_tensor = paddle.to_tensor(<span class="number">1</span>, place=paddle.CPUPlace())</span><br><span class="line"><span class="built_in">print</span>(cpu_tensor)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1], dtype=int64, place=Place(cpu), stop_gradient=True,       [1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建GPU上的Tensor</span></span><br><span class="line">gpu_tensor = paddle.to_tensor(<span class="number">1</span>, place=paddle.CUDAPlace(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(gpu_tensor)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=True,       [1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建固定内存上的Tensor</span></span><br><span class="line">pin_memory_tensor = paddle.to_tensor(<span class="number">1</span>, place=paddle.CUDAPinnedPlace())</span><br><span class="line"><span class="built_in">print</span>(pin_memory_tensor)</span><br></pre></td></tr></table></figure><pre><code>Tensor(shape=[1], dtype=int64, place=Place(gpu_pinned), stop_gradient=True,       [1])</code></pre><table><tr><td bgcolor=orange>王小美:</td></tr></table>好耶，tensor在我的电脑上反复横跳<p><img src="https://ai-studio-static-online.cdn.bcebos.com/48808d2902bb47afb6eee70948e67809e8b007296b6c4d2c9379fc5aee492cc1"></p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>好了今天的课就上到这了<table><tr><td bgcolor=orange>王小美:</td></tr></table>太好了，下课咯。我一定能成为炼丹大师的！<h1 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h1><p>ID：Flose(是不迷失的意思，拖延症有点严重想让自己自律点)</p><p>School：浙大宁波理工学院</p><p>专业：自动化</p><p>深度学习菜狗，正在不断努力，咱们一起加油</p><p>后记：</p><p>一些好的材料推荐：</p><ol><li><p><a href="https://aistudio.baidu.com/aistudio/personalcenter/thirdview/284366">跟三岁大佬一起学习(三岁paddle主页)</a>本教程以三岁大佬的教程为基础以对话形式呈现</p></li><li><p><a href="https://www.bilibili.com/video/BV1AL4y1Y7gu/?spm_id_from=333.999.0.0&vd_source=23eb3001d2bec3aba19d7fd7c10b4409">李沐老师的动手学深度学习</a></p></li><li><p><a href="https://aistudio.baidu.com/aistudio/course/introduce/25851?sharedType=1&sharedUserId=2413201&ts=1673084671791">动手学深度学习对应的paddle版本的电子书</a>推荐这两个配套学习</p></li><li><p>没有实战但是讲的很易懂<a href="https://www.bilibili.com/video/BV1Pa411X76s/?spm_id_from=333.337.search-card.all.click&vd_source=23eb3001d2bec3aba19d7fd7c10b4409">吴恩达的机器学习课程</a></p></li></ol><p>全国大学生智能车竞赛百度完全模型组火爆开赛中！！！<br>让我们在此相遇<br><img src="https://ai-studio-static-online.cdn.bcebos.com/64aea31fea484731b4c33de138ab06c82efd3d9958824bf0bc43334d7d805069"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机科学 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跟着王小美和三岁一起学paddle(2)--第一个机器学习程序</title>
      <link href="/2023/02/26/%E8%B7%9F%E7%8E%8B%E5%B0%8F%E7%BE%8E%E4%B8%8E%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle%20%E2%80%94%20%E7%8E%8B%E5%B0%8F%E7%BE%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/"/>
      <url>/2023/02/26/%E8%B7%9F%E7%8E%8B%E5%B0%8F%E7%BE%8E%E4%B8%8E%E4%B8%89%E5%B2%81%E4%B8%80%E8%B5%B7%E5%AD%A6paddle%20%E2%80%94%20%E7%8E%8B%E5%B0%8F%E7%BE%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="跟王小美与三岁一起学paddle-—-王小美的第一个机器学习程序"><a href="#跟王小美与三岁一起学paddle-—-王小美的第一个机器学习程序" class="headerlink" title="跟王小美与三岁一起学paddle — 王小美的第一个机器学习程序"></a>跟王小美与三岁一起学paddle — 王小美的第一个机器学习程序</h1><p> 简单有趣带你helloworld</p><p> 跟王小美与三岁一起学paddle 第二讲</p><blockquote><p>有的同学可能会疑惑，怎么没有讲广播机制？有关广播机制章节的内容我们会在用到的时候补充，敬请期待</p></blockquote><blockquote><p>致读者</p><p>在看这个notebook的你，对没错就是你</p><p>关注王小美喵，点个star⭐谢谢喵</p><p><img src="https://ai-studio-static-online.cdn.bcebos.com/30da901ef6ac4ce68ca9b269af8c1cb0d3bc9c2880054e0ab0f23f161eeaeab8"></p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>终于要开始学习深度学习了吗？ 我已经迫不及待了(p≧w≦q)<table><tr><td bgcolor=baby blue>三岁:</td></tr></table>来我给你出个简单的编程题￣ω￣=<blockquote><h1 id="出租车问题"><a href="#出租车问题" class="headerlink" title="出租车问题"></a>出租车问题</h1><p>我们乘坐出租车的时候，会有一个10元的起步价，只要上车就需要收取。出租车每行驶1公里，需要再支付每公里2元的行驶费用。当一个乘客坐完出租车之后，车上的计价器需要算出来该乘客需要支付的乘车费用。请你打印出乘客分别乘车从1公里到20公里费用</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table><p>这个简单我会 (★ᴗ★)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Kilometers = [<span class="number">1.0</span>, <span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">9.0</span>, <span class="number">10.0</span>, <span class="number">20.0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Kilometers:</span><br><span class="line">    money = <span class="number">10</span> + <span class="number">2</span>*i</span><br><span class="line">    <span class="built_in">print</span>(money)</span><br></pre></td></tr></table></figure><pre><code>12.016.020.028.030.050.0</code></pre><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>不错，看来你已经学会使用编程来解决问题了(★＞U＜★)<p>但是如果我没有告诉你计费规则，而是给你每个公里数对应的费用你们算出其他公里数对应的费用吗？\（￣︶￣）/</p><table><tr><td bgcolor=orange>王小美:</td></tr></table>啊这？ 我虽然可以自己找出规律，然后编写程序来解决这个问题，但是我不知道怎么让计算机找出规律，终究还是需要人脑的参与ε(┬┬﹏┬┬)3<p>░ ∗ ◕ ں ◕ ∗ ░ 所以三岁老师是不是要教我用机器学习的方法来解决这个问题了</p><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>是的，让我们学起来吧！<h1 id="使用深度学习方法解决问题"><a href="#使用深度学习方法解决问题" class="headerlink" title="使用深度学习方法解决问题"></a>使用深度学习方法解决问题</h1><blockquote><h1 id="深度学习的步骤一般为以下几步哦！"><a href="#深度学习的步骤一般为以下几步哦！" class="headerlink" title="深度学习的步骤一般为以下几步哦！"></a>深度学习的步骤一般为以下几步哦！</h1><p>1.数据处理</p><p>2.模型构建</p><p>3.训练调参</p><p>4.模型验证</p><p>5.模型部署</p></blockquote><h1 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1.数据处理"></a>1.数据处理</h1><blockquote><p>数据处理一般有 划分数据集 数据增强等</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入paddle</span></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">paddle.__version__</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编辑数据</span></span><br><span class="line">x_data = paddle.to_tensor([[<span class="number">1.</span>], [<span class="number">3.0</span>], [<span class="number">5.0</span>], [<span class="number">9.0</span>], [<span class="number">10.0</span>], [<span class="number">20.0</span>]])</span><br><span class="line">y_data = paddle.to_tensor([[<span class="number">12.</span>], [<span class="number">16.0</span>], [<span class="number">20.0</span>], [<span class="number">28.0</span>], [<span class="number">30.0</span>], [<span class="number">50.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_data = &quot;</span>,x_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_data = &quot;</span>,y_data)</span><br></pre></td></tr></table></figure><pre><code>x_data =  Tensor(shape=[6, 1], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[1. ],        [3. ],        [5. ],        [9. ],        [10.],        [20.]])y_data =  Tensor(shape=[6, 1], dtype=float32, place=Place(gpu:0), stop_gradient=True,       [[12.],        [16.],        [20.],        [28.],        [30.],        [50.]])</code></pre><h1 id="2-模型构建"><a href="#2-模型构建" class="headerlink" title="2.模型构建"></a>2.模型构建</h1><blockquote><h1 id="组网函数介绍"><a href="#组网函数介绍" class="headerlink" title="组网函数介绍"></a>组网函数介绍</h1><h1 id="paddle-nn"><a href="#paddle-nn" class="headerlink" title="paddle.nn"></a>paddle.nn</h1><p>paddle.nn 目录下包含飞桨框架支持的神经网络层和相关函数的相关API<br>详细内容请参考<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html">paddle的api文档</a></p></blockquote><blockquote><h1 id="本次使用线性变换层-Linear"><a href="#本次使用线性变换层-Linear" class="headerlink" title="本次使用线性变换层(Linear)"></a>本次使用线性变换层(Linear)</h1><h1 id="paddle-nn-Linear"><a href="#paddle-nn-Linear" class="headerlink" title="paddle.nn.Linear"></a>paddle.nn.Linear</h1><p>线性变换层。对于每个输入 Tensor X，计算公式为：</p><p>Out=XW+b</p><p>Linear 层只接受一个 Tensor 作为输入，形状为 [batch_size,∗,in_features]，其中 ∗ 表示可以为任意个额外的维度。 </p><p>该层可以计算输入 Tensor 与权重矩阵 W 的乘积，然后生成形状为 [batch_size,∗,out_features] 的输出 Tensor。 如果 bias_attr 不是 False，则将创建一个偏置参数并将其添加到输出中。</p></blockquote><blockquote><p>参数</p><p>in_features (int) – 线性变换层输入单元的数目。</p><p>out_features (int) – 线性变换层输出单元的数目。</p><p>weight_attr (ParamAttr，可选) – 指定权重参数的属性。默认值为 None，表示使用默认的权重参数属性，将权重参数初始化为 0。具体用法请参见 ParamAttr 。</p><p>bias_attr (ParamAttr|bool，可选) – 指定偏置参数的属性。bias_attr 为 bool 类型且设置为 False 时，表示不会为该层添加偏置。bias_attr 如果设置为 True 或者 &gt;<br>None，则表示使用默认的偏置参数属性，将偏置参数初始化为 0。具体用法请参见 ParamAttr。默认值为 None。</p><p>name (str，可选) - 具体用法请参见 Name，一般无需设置，默认值为 None。</p></blockquote><blockquote><p>属性</p><p>weight</p><p>本层的可学习参数，类型为 Parameter 。</p><p>bias</p><p>本层的可学习偏置，类型为 Parameter 。</p></blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table> 让我们来试试吧 ٩(◕‿◕｡)۶ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear = paddle.nn.Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>)  <span class="comment"># 定义初始化神经网络</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看初始化策略</span></span><br><span class="line"><span class="comment"># w 的值会先进行随机生成</span></span><br><span class="line"><span class="comment"># b 的值会先以0进行代替</span></span><br><span class="line"></span><br><span class="line">w_before_opt = linear.weight.numpy().item()  <span class="comment"># 获取w的值</span></span><br><span class="line">b_before_opt = linear.bias.numpy().item()  <span class="comment"># 获取b的值</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w before optimize: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(w_before_opt))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b before optimize: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(b_before_opt))</span><br></pre></td></tr></table></figure><pre><code>w before optimize: 0.21255211532115936b before optimize: 0.0</code></pre><blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>现在的神经网络类似于我们的无情答卷人疯狂做题但是有不知道对错，怎么办？？？</p><p>现在需要一个损失函数：相当于改卷人</p><p>需要一个优化策略：相当于看到错误进行反思然后进行修改的好孩子</p><p>损失函数就是预测值与结果的偏差</p></blockquote><blockquote><p>此处使用的：</p><p>损失函数(平均方差(mseloss))：paddle.nn.loss.MSELoss(reduction=’mean’) <a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html">参考地址</a></p><p>优化算法(随机梯度下降(SGD))：class paddle.optimizer.SGD(learning_rate=0.001, parameters=None, weight_decay=None, grad_clip=None, name=None) </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数和优化策略</span></span><br><span class="line">mse_loss = paddle.nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">sgd_optimizer = paddle.optimizer.SGD(learning_rate=<span class="number">0.001</span>, parameters = linear.parameters())</span><br></pre></td></tr></table></figure><h1 id="3-训练和调参"><a href="#3-训练和调参" class="headerlink" title="3.训练和调参"></a>3.训练和调参</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">total_epoch = <span class="number">10000</span>  <span class="comment"># 运行轮数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(total_epoch):</span><br><span class="line">    y_predict = linear(x_data)</span><br><span class="line">    loss = mse_loss(y_predict, y_data)</span><br><span class="line">    loss.backward() <span class="comment"># 自动对损失函数求导(梯度)</span></span><br><span class="line">    sgd_optimizer.step() <span class="comment"># 更新w和bias</span></span><br><span class="line">    sgd_optimizer.clear_grad() <span class="comment"># 每轮都清除一次导数(梯度)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">1000</span> == <span class="number">0</span>:  <span class="comment"># 每1000轮输出一次</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;epoch &#123;&#125; loss &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, loss.numpy()))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;finished training， loss &#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss.numpy()))</span><br></pre></td></tr></table></figure><pre><code>epoch 0 loss [0.00101089]epoch 1000 loss [0.00022613]epoch 2000 loss [5.0662904e-05]epoch 3000 loss [1.13599e-05]epoch 4000 loss [2.6291857e-06]epoch 5000 loss [6.5847786e-07]epoch 6000 loss [1.5586754e-07]epoch 7000 loss [1.5586754e-07]epoch 8000 loss [1.5586754e-07]epoch 9000 loss [1.5586754e-07]finished training， loss [1.5586754e-07]</code></pre><blockquote><table><tr><td bgcolor=baby blue>三岁:</td></tr></table>可以看到在7000 epoch 的时候loss就已经保持不变了所以在7000 epoch 的时候效果最好</blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.模型验证</span></span><br><span class="line">w_after_opt = linear.weight.numpy().item()</span><br><span class="line">b_after_opt = linear.bias.numpy().item()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;w after optimize: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(w_after_opt))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b after optimize: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(b_after_opt))</span><br></pre></td></tr></table></figure><pre><code>w after optimize: 2.0000507831573486b after optimize: 9.999356269836426</code></pre><table><tr><td bgcolor=baby blue>三岁:</td></tr></table><blockquote><p>我们来对一下答案  </p><p>b为bias 既出租车起步价 w为权重 既出租车每公里加价</p><p>原题目中 b为10 w为2<br>可以看到我们的输出结果已经非常接近了</p></blockquote><table><tr><td bgcolor=orange>王小美:</td></tr></table>原来如此谢谢三岁老师，原来机器是这样学习的啊 (￣▽￣)ノ<p>我以后也要炼出厉害的ai</p><h1 id="4-模型部署"><a href="#4-模型部署" class="headerlink" title="4.模型部署"></a>4.模型部署</h1><blockquote><p>这块内容可以通过看别人的部署项目来了解</p><p>这边推荐大家一个paddle的快速部署套件<a href="https://github.com/PaddlePaddle/FastDeploy">fastdeploy</a></p></blockquote><h1 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h1><p>ID：Flose(是不迷失的意思，拖延症有点严重想让自己自律点)</p><p>School：浙大宁波理工学院</p><p>专业：自动化</p><p>深度学习菜狗，正在不断努力，咱们一起加油</p>]]></content>
      
      
      <categories>
          
          <category> 计算机科学 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cs学习指南</title>
      <link href="/2023/02/26/cs%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/"/>
      <url>/2023/02/26/cs%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h1 id="文章来源于-https-github-com-sanbuphy-my-awesome-cs"><a href="#文章来源于-https-github-com-sanbuphy-my-awesome-cs" class="headerlink" title="文章来源于 https://github.com/sanbuphy/my-awesome-cs"></a>文章来源于 <a href="https://github.com/sanbuphy/my-awesome-cs">https://github.com/sanbuphy/my-awesome-cs</a></h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>收录了个人喜好觉得品味好的网站（也有一些小工具），</p><p>如果有其他好的建议也欢迎提出，非常感谢。</p><p>不定期更新</p><p>带目录边栏（电脑上可看）：<a href="https://sanbuphy.github.io/p/%E6%88%91%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%9B%86%E5%90%88/">https://sanbuphy.github.io/p/我的计算机学习网站集合/</a></p><p>github开源仓库地址： <a href="https://github.com/sanbuphy/my-awesome-cs">https://github.com/sanbuphy/my-awesome-cs</a></p><p><strong>基础素质要求（自勉用，参考NJUPA内的要求）</strong></p><p>提问的艺术</p><p><a href="https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md">https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md</a></p><p>不像弱智一样提问</p><p><a href="https://github.com/tangx/Stop-Ask-Questions-The-Stupid-Ways/blob/master/README.md">https://github.com/tangx/Stop-Ask-Questions-The-Stupid-Ways/blob/master/README.md</a></p><p><strong>部分内容出自以下参考网站，也欢迎关注他们</strong></p><p>PPRP：</p><p><a href="https://www.cnblogs.com/pprp/p/8880493.html">https://www.cnblogs.com/pprp/p/8880493.html</a></p><p><strong>如需转载请注释原出处即可，谢谢</strong></p><h2 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h2><p>机器学习相关数学基础</p><p><a href="https://www.bilibili.com/video/BV1iW411T781?p=34&share_medium=iphone&share_plat=ios&share_session_id=918760D9-B272-4504-8DD6-82E44AFA8672&share_source=WEIXIN&share_tag=s_i&timestamp=1641652920&unique_k=aGspGLd">直观理解机器学习的数学过程</a></p><p><a href="https://www.bilibili.com/video/BV1xk4y1B7RQ?p=1&share_medium=iphone&share_plat=ios&share_session_id=1F101D5C-2880-4C53-A556-3D2777F6AFC8&share_source=WEIXIN&share_tag=s_i&timestamp=1641653070&unique_k=GcJNM2u">矩阵求导入门</a> 或者你也可以参考我整理的文章：</p><p><a href="https://sanbuphy.github.io/p/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E7%AE%80%E6%98%93%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/">https://sanbuphy.github.io/p/矩阵求导简易入门手册/</a></p><p><a href="https://www.bilibili.com/video/BV1o5411p7H2?p=8&share_medium=iphone&share_plat=ios&share_session_id=3AD6E589-C577-4D7A-88C6-6A3FAB1E41F1&share_source=WEIXIN&share_tag=s_i&timestamp=1641653147&unique_k=abvhCWL">李航统计学习基础第一章补数学基础</a> 只需要第一张 补基础，其他有问题再找</p><p><a href="https://www.deeplearningbook.org/">Deep Learning An MIT Press book</a>参考第一章即可，<a href="https://github.com/exacity/deeplearningbook-chinese/releases">中文版在这</a>或者直接下载附件中dlbook_cn_v0.5-beta。</p><h3 id="概率论与数理统计"><a href="#概率论与数理统计" class="headerlink" title="概率论与数理统计"></a>概率论与数理统计</h3><p>陈希孺 概率论与数理统计基础 <a href="https://www.bilibili.com/video/BV12k4y1m78w">参考课程视频地址</a></p><p>【概率统计课程学习总结】1. 台大概率与台湾交通大学统计课 - 奶油煎蛋红烧肉的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/86071634">https://zhuanlan.zhihu.com/p/86071634</a></p><p><a href="https://www.bilibili.com/video/BV1nK4y1U7QM">台湾大学 - 頑想學概率：機率一 (Probability (1))</a></p><p><a href="https://www.bilibili.com/video/BV1CX4y1V7oN?p=23">台湾大学 - 頑想學概率：機率二 (Probability (2))</a></p><p><a href="https://ocw.nctu.edu.tw/course_detail-v.php?bgid=1&gid=1&nid=270">台湾交通大学 - 統計學 Statistics</a></p><p><a href="https://ocw.nctu.edu.tw/course_detail-v.php?bgid=1&gid=4&nid=536">台湾交通大学 - 高等統計學 Advanced Statistics</a></p><p>其他</p><p>线代启示录（一位掌握了线代灵魂的老师）</p><p><a href="https://ccjou.wordpress.com/">https://ccjou.wordpress.com/</a></p><p>immersive linear algebra 线性代数可视化</p><p><a href="http://immersivemath.com/ila/index.html">http://immersivemath.com/ila/index.html</a></p><h2 id="CS大类"><a href="#CS大类" class="headerlink" title="CS大类"></a>CS大类</h2><p>CS自学指南【必看】</p><p><a href="https://csdiy.wiki/">https://csdiy.wiki/</a></p><p>【北美名校CS课程集锦】2.加州大学伯克利分校CS课程全集 - 文兄的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/102083014">https://zhuanlan.zhihu.com/p/102083014</a></p><p>Quick Reference开发人员速查表（各种语言、脚本、常用工具的命令速查）</p><p><a href="https://quickref.me/">https://quickref.me/</a></p><p><a href="https://xushanxiang.com/ref/">https://xushanxiang.com/ref/</a></p><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><h4 id="GDB、VIM、GIT、SHELL等常见linux操作基础（慢慢来，在使用中学"><a href="#GDB、VIM、GIT、SHELL等常见linux操作基础（慢慢来，在使用中学" class="headerlink" title="GDB、VIM、GIT、SHELL等常见linux操作基础（慢慢来，在使用中学"></a>GDB、VIM、GIT、SHELL等常见linux操作基础（慢慢来，在使用中学</h4><ul><li>The Missing Semester of Your CS Education 中文版（强烈推荐）<ul><li>官方中文站点：<a href="https://missing-semester-cn.github.io/">https://missing-semester-cn.github.io/</a></li><li>B站：<a href="https://www.bilibili.com/video/BV1x7411H7wa?t=2829">https://www.bilibili.com/video/BV1x7411H7wa?t=2829</a></li></ul></li><li>南大PA教程最下面的一些简单入门和材料</li></ul><p><a href="https://nju-projectn.github.io/ics-pa-gitbook/ics2021/index.html">https://nju-projectn.github.io/ics-pa-gitbook/ics2021/index.html</a></p><ul><li>命令行的艺术（总结了各种命令行下相关好物）<ul><li><a href="https://github.com/jlevy/the-art-of-command-line">https://github.com/jlevy/the-art-of-command-line</a></li></ul></li><li>如何学习写shell脚本（封装）可参考的项目：</li></ul><p><a href="https://github.com/sanbuphy/vimplus/blob/master/install.sh">https://github.com/sanbuphy/vimplus/blob/master/install.sh</a></p><ul><li>shell和程序开头的字符画LOGO生成工具</li></ul><p><a href="https://tools.kalvinbg.cn/txt/ascii">https://tools.kalvinbg.cn/txt/ascii</a></p><h4 id="系统相关及系统信息相关"><a href="#系统相关及系统信息相关" class="headerlink" title="系统相关及系统信息相关"></a>系统相关及系统信息相关</h4><p>如何开机自动挂载新硬盘（非ubuntu安装硬盘）</p><p><a href="https://blog.csdn.net/qq_27370437/article/details/117806294">https://blog.csdn.net/qq_27370437/article/details/117806294</a></p><p>获取CPU的性能信息和常见信息，涵盖x86/arm等多硬件</p><p><a href="https://github.com/pytorch/cpuinfo">https://github.com/pytorch/cpuinfo</a></p><p>如何维护和切换gcc与g++代码（两种方案）</p><p>最简单的，通过设置不同优先级并切换即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.cnblogs.com/haiyonghao/p/14440283.html</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 40</span><br><span class="line"><span class="comment"># 设置gcc-5的优先级</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 50</span><br><span class="line"><span class="comment"># 修改系统中的默认gcc版本</span></span><br><span class="line">sudo update-alternatives --config gcc</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果想要根据优先级切换gcc版本，可以输入这个后在输入前面设定的数字</span></span><br><span class="line">sudo update-alternatives --config gcc</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/FX677588/article/details/78681325">https://blog.csdn.net/FX677588/article/details/78681325</a></p><p><a href="https://cloud.tencent.com/developer/article/1430839">https://cloud.tencent.com/developer/article/1430839</a></p><p>如何及时获取可视化的CPU频率和使用率信息：(Linux 查看CPU当前运行频率和温度)</p><p>sudo pip install s-tui</p><p>sudo s-tui</p><h4 id="vscode相关"><a href="#vscode相关" class="headerlink" title="vscode相关"></a>vscode相关</h4><ul><li>自动格式化python代码</li></ul><p>首先安装google的格式化程序：<code>pip install yapf</code></p><p>然后键入<code>ctrl+shift+p</code>然后输入<code>settings</code>往下找到用户的json，如果你是WSL的话就要找到对应WSL的config，然后输入<code> &quot;python.formatting.provider&quot;: &quot;yapf&quot;</code>。之后只要使用<code>Alt+shift+F</code>即可格式化整体的python项目代码。</p><h4 id="git相关"><a href="#git相关" class="headerlink" title="git相关"></a>git相关</h4><ul><li>git常见操作整理</li></ul><p><a href="https://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html">https://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html</a></p><ul><li>简单的git ssh秘钥教程</li></ul><p><a href="https://blog.csdn.net/helloasimo/article/details/123778112">https://blog.csdn.net/helloasimo/article/details/123778112</a></p><ul><li>添加了SSH密钥，git推送/拉取时要求验证</li></ul><p><a href="https://blog.csdn.net/qq_37435462/article/details/122240506">https://blog.csdn.net/qq_37435462/article/details/122240506</a></p><ul><li>简单的pr教程</li></ul><p><a href="https://mmcv.readthedocs.io/zh_CN/latest/community/pr.html">https://mmcv.readthedocs.io/zh_CN/latest/community/pr.html</a></p><ul><li>github linux下的desktop版</li></ul><p><a href="https://github.com/shiftkey/desktop/releases">https://github.com/shiftkey/desktop/releases</a></p><ul><li>git的rebase的使用说明（可用于合并多次commit,修改commit等情况）</li></ul><p><a href="https://www.jianshu.com/p/4a8f4af4e803">https://www.jianshu.com/p/4a8f4af4e803</a></p><h4 id="ubuntu常见疑难解答"><a href="#ubuntu常见疑难解答" class="headerlink" title="ubuntu常见疑难解答"></a>ubuntu常见疑难解答</h4><ul><li><p>快速下载ubuntu镜像: 找到官网下载链接后使用wget下载</p></li><li><p>简单安装双系统</p><p>  1、安装windows（因为windows的boot优先级比较高）</p><p>  2、安装ubuntu，他能检测到和windows并存的状态，选择那个安装即可</p></li><li><p>简单更换ubuntu镜像源     </p><ul><li>Settings→About→Software Updates→Download from </li><li>选择其他服务器，然后找到中国，选择进行测试以便找到最快的站点。</li></ul></li><li><p>sudo apt-get update: 0% [正在等待报头]问题的解决（参考<a href="https://article.itxueyuan.com/XP2rn">https://article.itxueyuan.com/XP2rn</a>）</p><ul><li>先断网然后找到Settings→About→Software Updates关闭所有下载</li><li>sudo apt-get clean</li><li>接下来将/etc/apt/source.list文件内容清空并保存</li><li>恢复网络，将第一步中取消掉的四个选项重新点选然后在最佳国内服务器更新即可。</li></ul></li></ul><h4 id="WSL2常见疑难解答"><a href="#WSL2常见疑难解答" class="headerlink" title="WSL2常见疑难解答"></a>WSL2常见疑难解答</h4><ul><li>WSL入坑指南（很全的资料，包括开发环境的配置）</li></ul><p><a href="https://dowww.spencerwoo.com/">https://dowww.spencerwoo.com/</a></p><ul><li>windows上安装ubuntu(WSL2)： </li></ul><p>1、在microsoft下载ubuntu 2、根据下列方式导出并导入镜像，防止占用C盘空间（默认安装在C盘）<a href="http://t.zoukankan.com/davidchild-p-15606786.html">http://t.zoukankan.com/davidchild-p-15606786.html</a>   （用这个方法还可以及时快照保存~</p><ul><li>WSL2常用命令（开启关闭甚至注销等等</li></ul><p><a href="https://learn.microsoft.com/zh-cn/windows/wsl/basic-commands">https://learn.microsoft.com/zh-cn/windows/wsl/basic-commands</a></p><ul><li>安装WSL2专用systemctl【目前（2023/02/02）只能用于20.04,请勿在22.04中使用！】（慎重使用，可能会导致pycharm连接出现问题，请先快照后再进行尝试！</li></ul><p><a href="https://github.com/DamionGans/ubuntu-wsl2-systemd-script">https://github.com/DamionGans/ubuntu-wsl2-systemd-script</a></p><ul><li>WSL中如何使用win v2ray的proxy：（直接在wsl里面跑即可）<ul><li>第一步 安装：<a href="https://github.com/v2fly/fhs-install-v2ray">https://github.com/v2fly/fhs-install-v2ray</a>（安装后其他步骤参考<a href="https://gukaifeng.cn/posts/linux-pei-zhi-v2ray-he-proxychains-shi-xian-ming-ling-xing-dai-li-wu-tu-xing-jie-mian/#1-3-%E5%90%AF%E5%8A%A8-V2Ray">https://gukaifeng.cn/posts/linux-pei-zhi-v2ray-he-proxychains-shi-xian-ming-ling-xing-dai-li-wu-tu-xing-jie-mian/#1-3-启动-V2Ray</a></li><li>第二步 启动：（因为WSL无法用systemctl，所以直接运行即可（或者自己去安装一下wsl2的systemctl），你可以后台运行，也可以在一个终端中运行起来，然后新开一个终端去export ALLproxy之类的就好，参考docker的做法，或者使用proxychains4也可以。）在终端中运行<code>/usr/local/bin/v2ray run -config /usr/local/etc/v2ray/config.json</code> 即可启动！</li><li>第三步 使用：就当作一个已经监听了某个端口的proxy使用即可</li><li>注释：当然，为了方便你可以自行改造，使用 /etc/init.d/ 目录中的服务命令或 service 命令替代systemctl。</li></ul></li><li>注意！WSL2经常会与主机时间不同步，这可能会造成很多问题（包括proxy用不了），你最好设置一个启动脚本或自己执行<code>sudo hwclock -s</code>强制对WSL2时间进行同步，可使用<code>sudo hwclock</code>查看WSL的当前时间，</li><li>以防出现奇怪问题你可以经常对WSL做快照：<a href="https://blog.csdn.net/weixin_43425561/article/details/115765148">https://blog.csdn.net/weixin_43425561/article/details/115765148</a></li><li>改变/增加WSL2的内存、swap空间大小</li></ul><p>在C盘的用户主目录下（比如我的是C:\Users\sanbu）创建<code>.wslconfig</code>文件夹，在里面输入</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[wsl2]</span><br><span class="line">memory=8GB</span><br><span class="line">swap=16GB</span><br><span class="line"># 为 WSL 2 虚拟机分配的处理器核心数量</span><br><span class="line"># processors=&lt;number&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在配置设置完之后，在powershell执行<code>wsl --shutdown</code>命令进行关闭，随后再打开即可（注意，如果你使用了docker-desktop，实际上也是基于wsl2构建的，所以也会影响到的对应环境的资源</p><h4 id="windows常见工具箱"><a href="#windows常见工具箱" class="headerlink" title="windows常见工具箱"></a>windows常见工具箱</h4><ul><li>有关win家的镜像源以及VS等的纯净安装文件，以及各种网络工程师能用到的软件程序安装包</li></ul><p><a href="https://msdn.itellyou.cn/">https://msdn.itellyou.cn/</a></p><ul><li>图吧工具箱（给自己电脑做硬件分析等等</li></ul><p><a href="http://www.tbtool.cn/">http://www.tbtool.cn/</a></p><ul><li>DISM++ 最好用的windows控制面板工具箱（直接看release部分下载</li></ul><p><a href="https://github.com/Chuyu-Team/Dism-Multi-language">https://github.com/Chuyu-Team/Dism-Multi-language</a></p><ul><li>Ubuntu - bash脚本与 Windows - bat 脚本互相转换工具（Bash to Bat Converter</li></ul><p><a href="https://daniel-sc.github.io/bash-shell-to-bat-converter/">https://daniel-sc.github.io/bash-shell-to-bat-converter/</a></p><ul><li>windows-linux命令行对照表</li></ul><p><a href="https://www.geeksforgeeks.org/linux-vs-windows-commands/">https://www.geeksforgeeks.org/linux-vs-windows-commands/</a></p><h4 id="正则表达式相关"><a href="#正则表达式相关" class="headerlink" title="正则表达式相关"></a>正则表达式相关</h4><ul><li>正则表达式入门与练习</li></ul><p><a href="https://github.com/ziishaned/learn-regex/blob/master/translations/README-cn.md">https://github.com/ziishaned/learn-regex/blob/master/translations/README-cn.md</a></p><ul><li>正则表达式可视化浏览</li></ul><p><a href="https://regexr.com/">https://regexr.com/</a></p><ul><li>长正则表达式结构可视化</li></ul><p><a href="https://regexper.com/">https://regexper.com/</a></p><ul><li>常用正则表达式汇总（车牌号手机号姓名IP等等）</li></ul><p><a href="http://obkoro1.com/web_accumulate/codeBlack/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%94%B6%E9%9B%86.html">http://obkoro1.com/web_accumulate/codeBlack/正则表达式收集.html</a></p><h4 id="docker相关"><a href="#docker相关" class="headerlink" title="docker相关"></a>docker相关</h4><ul><li>docker的一切：</li></ul><p><a href="https://yeasy.gitbook.io/docker_practice/">https://yeasy.gitbook.io/docker_practice/</a></p><ul><li>在WSL2下使用docker</li></ul><p>【你只需要安装docker desktop然后参考这个教程，点点就能用了】：</p><p><a href="https://dockerdocs.cn/docker-for-windows/wsl/">https://dockerdocs.cn/docker-for-windows/wsl/</a></p><p>可以很方便使用！无需按照命令行安装即可使用docker！</p><ul><li>安装docker后启动时遇到：Failed to connect to bus: Host is down（我在WSL2遇到，非必要不使用，最好还是先安装一下systemctl确保能用）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行下列操作即可，然后 sudo systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># https://gist.github.com/alyleite/ca8b10581dbecd722d9dcc35b50d9b2b</span></span><br><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install -yqq daemonize dbus-user-session fontconfig</span><br><span class="line"></span><br><span class="line">sudo daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit=basic.target</span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> sudo nsenter -t $(pidof systemd) -a su - <span class="variable">$LOGNAME</span></span><br><span class="line"></span><br><span class="line">snap version</span><br></pre></td></tr></table></figure><ul><li>使docker能够避免输入sudo（通过 docker info检查是否要sudo才可输出）</li></ul><p><a href="https://www.yisu.com/zixun/139260.html">https://www.yisu.com/zixun/139260.html</a></p><ul><li>NVIDIA docker——nvidia-docker2的相关安装</li></ul><p><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker</a></p><ul><li>docker换源（拉镜像极大加速）</li></ul><p><a href="https://blog.51cto.com/u_13281972/2997681">https://blog.51cto.com/u_13281972/2997681</a></p><ul><li>修改Docker默认镜像和容器的存储位置</li></ul><p><a href="https://www.cnblogs.com/chentiao/p/16963647.html">https://www.cnblogs.com/chentiao/p/16963647.html</a></p><ul><li>docker  pull images — use proxy </li></ul><p><a href="https://www.lfhacks.com/tech/pull-docker-images-behind-proxy/">https://www.lfhacks.com/tech/pull-docker-images-behind-proxy/</a></p><ul><li>docker — use proxy（在容器内）</li></ul><p><strong>方法一：</strong></p><p>-it 以及加上了host命令进入docker后（比如：）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-docker run --name paddle-test -v <span class="variable">$PWD</span>:/paddle --network=host -it [registry.baidubce.com/paddlepaddle/paddle:latest-gpu-cuda10.2-cudnn7-dev](http://registry.baidubce.com/paddlepaddle/paddle:latest-gpu-cuda10.2-cudnn7-dev) /bin/bash</span><br></pre></td></tr></table></figure><p>此时在内部可以看到两个网卡，我们可以监听172ip的某个端口，然后使用</p><p>export ALL_PROXY=”<a href="http://172.17.0.1:8888/">http://172.17.0.1:8888/</a>“</p><p><code>export ALL_PROXY=socks5://172.17.0.1:1088</code> 即可使用proxy。（有时候还不够用，可以加上https的）（不需要host network 只需要bind 172即可使用）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> http_proxy=<span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HTTP_PROXY=<span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> https_proxy=<span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HTTPS_PROXY=<span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br></pre></td></tr></table></figure><p><strong>方法二：</strong></p><ol><li>make sure your proxy bind 172.17.0.1 and port  (e.g. 8888)</li><li>add that in dockerfile</li></ol><figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENV</span> http_proxy <span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> HTTP_PROXY <span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> https_proxy <span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> HTTPS_PROXY <span class="string">&quot;http://172.17.0.1:8888/&quot;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>run it </li></ol><p><strong>方法三：</strong>运行终端版的v2ray之类的软件，后台运行后直接使用proxychains4的http代理即可（很多不能走socker）</p><p>注释：如果而在终端遇到curl之类的奇怪的http问题，请env|grep查看有无奇怪的环境变量或者关闭proxy的系统proxy功能。因为无需开启也可以操作。</p><ul><li>docker磁盘占用查看与缓存清理</li></ul><p><a href="https://blog.csdn.net/m0_67390963/article/details/126327604">https://blog.csdn.net/m0_67390963/article/details/126327604</a></p><ul><li>利用docker调试代码，以apollo为例：</li></ul><p><a href="https://zhuanlan.zhihu.com/p/468146522">https://zhuanlan.zhihu.com/p/468146522</a></p><ul><li>导出自己的镜像和导入：</li></ul><p><a href="https://yeasy.gitbook.io/docker_practice/container/import_export">https://yeasy.gitbook.io/docker_practice/container/import_export</a></p><ul><li>提交自己的镜像到dockehub：(你也可以参考上面提到的docker的一切，里面什么都有。</li></ul><p>可参考 <a href="https://blog.csdn.net/butterfly5211314/article/details/83068807">https://blog.csdn.net/butterfly5211314/article/details/83068807</a></p><p>不知道变量怎么命名就可以看看：</p><p><a href="https://unbug.github.io/codelf/">https://unbug.github.io/codelf/</a></p><h3 id="其他有趣的文章"><a href="#其他有趣的文章" class="headerlink" title="其他有趣的文章"></a>其他有趣的文章</h3><p>有关linux的基础讲解，有配图和自己的理解，推荐一读。</p><p><a href="https://segmentfault.com/u/public0821">https://segmentfault.com/u/public0821</a></p><p>一个对cpu和网络了解都非常深入的工程师</p><p><a href="https://plantegg.github.io/">https://plantegg.github.io/</a></p><p>其中最好的一类文章（有关cpu的讲解）<a href="https://plantegg.github.io/2021/06/01/CPU%E7%9A%84%E5%88%B6%E9%80%A0%E5%92%8C%E6%A6%82%E5%BF%B5/">https://plantegg.github.io/2021/06/01/CPU的制造和概念/</a></p><p>一个关于各种生成网络和编码器小论文通读的博主，有些写的还可以（比较基础入门）</p><p><a href="https://medium.com/@falconives">https://medium.com/@falconives</a></p><p>java相关技术栈资料大全博主（还有一些三大件相关的资料，还挺多</p><p><a href="http://learn.lianglianglee.com/">http://learn.lianglianglee.com/</a></p><p>Linux性能分析工具大全（Linux/BSD性能专家Brendan Gregg）</p><p><a href="https://www.brendangregg.com/linuxperf.html">https://www.brendangregg.com/linuxperf.html</a></p><p>面向程序员的各类调用库清单（主要是C/C++ PYTHON)</p><p><a href="https://github.com/programthink/opensource">https://github.com/programthink/opensource</a></p><p>GitHub中文排行榜</p><p><a href="https://github.com/GrowingGit/GitHub-Chinese-Top-Charts">https://github.com/GrowingGit/GitHub-Chinese-Top-Charts</a></p><p>GitHub 上有趣、入门级的开源项目</p><p><a href="https://github.com/521xueweihan/HelloGitHub">https://github.com/521xueweihan/HelloGitHub</a></p><p>美化自己的github界面</p><p><a href="https://zhuanlan.zhihu.com/p/454597068">https://zhuanlan.zhihu.com/p/454597068</a></p><p><a href="http://github.com/rzashakeri/beautify-github-profile">github.com/rzashakeri/beautify-github-profile</a> </p><p><a href="https://bowenyoung.cn/posts/githubBeautify">https://bowenyoung.cn/posts/githubBeautify</a></p><p>社区制作的一键生成界面：<a href="https://rahuldkjain.github.io/gh-profile-readme-generator/">https://rahuldkjain.github.io/gh-profile-readme-generator/</a></p><p>公众号 / 真没什么逻辑的作者（为什么这么设计系列文章）涉及网络、数据库、操作系统等</p><p><a href="https://draveness.me/whys-the-design/">https://draveness.me/whys-the-design/</a></p><p>Roadmap to becoming a developer</p><p><a href="https://github.com/kamranahmedse/developer-roadmap">https://github.com/kamranahmedse/developer-roadmap</a></p><p>小林 x 图解计算机基础（国内最好的八股文整理之一）（图解网络和操作系统）</p><p><a href="https://xiaolincoding.com/">https://xiaolincoding.com/</a></p><p>linux inside  讲解了一下linux内部运行机制，比如Program startup process in userspace</p><p><a href="https://0xax.gitbooks.io/linux-insides/content/Misc/linux-misc-4.html">https://0xax.gitbooks.io/linux-insides/content/Misc/linux-misc-4.html</a></p><p>苏剑林的空间（从数学再到NLP再到天文都有涉猎</p><p><a href="https://spaces.ac.cn/">https://spaces.ac.cn/</a></p><h3 id="操作系统与体系结构"><a href="#操作系统与体系结构" class="headerlink" title="操作系统与体系结构"></a>操作系统与体系结构</h3><p>南京大学计算机基础（袁春风）CSAPP的青春版，但比csapp好懂得多（强烈不建议一开始就读csapp</p><p>赶时间可以直接看配套书。</p><p><a href="https://www.icourse163.org/course/nju-1001625001#/info">https://www.icourse163.org/course/nju-1001625001#/info</a></p><p>前置：南京大学计算机基础实验（做了能让你真的变强）<a href="https://nju-projectn.github.io/ics-pa-gitbook/ics2021/index.html">https://nju-projectn.github.io/ics-pa-gitbook/ics2021/index.html</a></p><p>2022 南京大学拔尖计划《操作系统：设计与实现》 </p><p>(蒋炎岩 我永远的超级无敌酷炫宝藏男神，还有什么好说的呢？没有他我就永远不懂计算机的美丽 </p><p>当然包云岗老师也是我的男神哈哈哈哈哈哈)</p><p>课程主页：<a href="http://jyywiki.cn/OS/2022/">http://jyywiki.cn/OS/2022/</a> (slides、示例代码)</p><p>视频地址： <a href="https://www.bilibili.com/video/BV1Cm4y1d7Ur/">https://www.bilibili.com/video/BV1Cm4y1d7Ur/</a> </p><p>操作系统（哈工大李治军老师）课件可在下方链接获取。</p><ul><li>慕课网: <a href="http://www.feemic.cn/mooc/icourse163/1002692015#%E3%80%82">http://www.feemic.cn/mooc/icourse163/1002692015#。</a></li><li>百度云链接：<a href="https://pan.baidu.com/s/1h2aEk6A_DGpXkZvRtNmeUw">https://pan.baidu.com/s/1h2aEk6A_DGpXkZvRtNmeUw</a> 提取码：qoll</li><li>配套实验课：<a href="https://www.shiyanlou.com/courses/115">https://www.shiyanlou.com/courses/115</a></li></ul><p>MIT 6.S081: Operating System Engineering</p><p><a href="https://csdiy.wiki/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/MIT6.S081/">https://csdiy.wiki/操作系统/MIT6.S081/</a></p><p>浙江大学周亚金老师的操作系统课课件（写的很好，有操作和现代的一些规范备注，我很喜欢）</p><p>在Schedule中可以获取到全部课件</p><p><a href="https://yajin.org/os2018fall/">https://yajin.org/os2018fall/</a></p><p>南京大学软件分析课程：</p><p><a href="https://tai-e.pascal-lab.net/pa1.html#_1-%E4%BD%9C%E4%B8%9A%E5%AF%BC%E8%A7%88">https://tai-e.pascal-lab.net/pa1.html#_1-作业导览</a></p><p><a href="https://space.bilibili.com/2919428/channel/series">https://space.bilibili.com/2919428/channel/series</a></p><p>教科书《计算机体系结构基础》（胡伟武等，第三版）的开源版本</p><p><a href="https://github.com/foxsen/archbase">https://github.com/foxsen/archbase</a></p><p>【MIT公开课】6.172 软件性能工程</p><p><a href="https://www.bilibili.com/video/BV1wA411h7N7/">https://www.bilibili.com/video/BV1wA411h7N7/</a></p><p><strong>其他有趣文章:</strong></p><p>如何实现一个elf的loader：<a href="https://blog.csdn.net/GoolyOh/article/details/119801160">https://blog.csdn.net/GoolyOh/article/details/119801160</a></p><p>从一个ELF程序的加载窥探操作系统内核:</p><p><a href="https://blog.csdn.net/goolyoh/category_11298420.html">https://blog.csdn.net/goolyoh/category_11298420.html</a></p><p>如何实现最小的hello world?</p><p><a href="https://cjting.me/2020/12/10/tiny-x64-helloworld/">https://cjting.me/2020/12/10/tiny-x64-helloworld/</a></p><p>Linux 内核揭密（一系列关于 Linux 内核和其内在机理的帖子。）</p><p><a href="https://xinqiu.gitbooks.io/linux-inside-zh/content/">https://xinqiu.gitbooks.io/linux-inside-zh/content/</a></p><h3 id="计算机网络学习"><a href="#计算机网络学习" class="headerlink" title="计算机网络学习"></a>计算机网络学习</h3><p>课程类待补充</p><p>其他文章：</p><p>tcp高级疑难汇总案例分析：<a href="http://plantegg.github.io/2021/02/14/TCP%E7%96%91%E9%9A%BE%E9%97%AE%E9%A2%98%E6%A1%88%E4%BE%8B%E6%B1%87%E6%80%BB/">plantegg.github.io/2021/02/14/TCP疑难问题案例汇总/</a></p><p>这个博主写了网络编程相关的一系列文章：<a href="https://juejin.cn/user/862486453028888/posts">https://juejin.cn/user/862486453028888/posts</a></p><p>其中我最喜欢：Nginx一网打尽：动静分离、压缩、缓存、黑白名单、跨域、高可用、性能优化：</p><p><a href="https://juejin.cn/post/7112826654291918855">https://juejin.cn/post/7112826654291918855</a></p><p>有前端Nginx服务器在线配置，及大改善修改nginx的配置体验</p><p><a href="https://www.digitalocean.com/community/tools/nginx?global.app.lang=zhCN">https://www.digitalocean.com/community/tools/nginx?global.app.lang=zhCN</a></p><p>项目来源：<a href="https://github.com/digitalocean/nginxconfig.io">https://github.com/digitalocean/nginxconfig.io</a></p><h3 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h3><p>程序员如何准备面试中的算法</p><p><a href="https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/00.01.html">https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/00.01.html</a></p><p>labuladong 的算法小抄</p><p><a href="https://github.com/labuladong/fucking-algorithm">https://github.com/labuladong/fucking-algorithm</a></p><p>ACWING的课</p><p><a href="https://www.acwing.com/activity/">https://www.acwing.com/activity/</a></p><p>GitHub’s largest open-source algorithm library</p><p><a href="https://the-algorithms.com/">https://the-algorithms.com/</a></p><p>数据结构与算法可视化</p><p><a href="https://visualgo.net/zh">https://visualgo.net/zh</a></p><h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p>一个图文并茂的设计模式学习网站</p><p><a href="http://refactoringguru.cn/design-patterns">refactoringguru.cn/design-patterns</a> </p><h2 id="深度学习大类"><a href="#深度学习大类" class="headerlink" title="深度学习大类"></a>深度学习大类</h2><h3 id="理论基础（时间少直接看李沐-李宏毅）"><a href="#理论基础（时间少直接看李沐-李宏毅）" class="headerlink" title="理论基础（时间少直接看李沐/李宏毅）"></a>理论基础（时间少直接看李沐/李宏毅）</h3><p><strong>周志华</strong></p><p>南瓜书主页</p><p><a href="https://datawhalechina.github.io/pumpkin-book/#/">https://datawhalechina.github.io/pumpkin-book/#/</a></p><p>周志华《机器学习》手推笔记 by Sophia-11</p><p> <a href="https://github.com/Sophia-11/Machine-Learning-Notes">https://github.com/Sophia-11/Machine-Learning-Notes</a></p><p>周志华《机器学习》笔记（主要是文本） by yv.l1.pnn</p><p> <a href="https://zhuanlan.zhihu.com/p/134089340">https://zhuanlan.zhihu.com/p/134089340</a></p><h3 id="李宏毅相关课程"><a href="#李宏毅相关课程" class="headerlink" title="李宏毅相关课程"></a><strong>李宏毅相关课程</strong></h3><ul><li>李宏毅老师的课程主页：</li></ul><p><a href="https://speech.ee.ntu.edu.tw/~hylee/index.php">https://speech.ee.ntu.edu.tw/~hylee/index.php</a> 这是李老师的个人主页，可以找到每年ML的课程主页，然后获取作业代码和Kaggle链接  </p><ul><li>李宏毅《机器学习》：  </li></ul><p><a href="https://www.bilibili.com/video/BV1Ht411g7Ef">https://www.bilibili.com/video/BV1Ht411g7Ef</a>  </p><ul><li>李宏毅机器学习笔记：  </li></ul><p><a href="https://gitee.com/datawhalechina/leeml-notes">https://gitee.com/datawhalechina/leeml-notes</a>  </p><ul><li>李宏毅《机器学习/深度学习》2021课程：</li></ul><p><a href="https://www.bilibili.com/video/BV1JA411c7VT?p=34">https://www.bilibili.com/video/BV1JA411c7VT?p=34</a>  </p><ul><li>李宏毅2022课程：  </li></ul><p><a href="https://www.bilibili.com/video/BV1JK4y1D7Wb/">https://www.bilibili.com/video/BV1JK4y1D7Wb/</a></p><p>李沐动手学深度学习（适合速成，打基础建议李宏毅）</p><p><a href="https://zh.d2l.ai/index.html">https://zh.d2l.ai/index.html</a></p><p>李沐深度学习精读</p><p><a href="https://github.com/mli/paper-reading">https://github.com/mli/paper-reading</a></p><p>这个网站给出了不同模型的排名及其开源代码</p><p><a href="https://paperswithcode.com/">https://paperswithcode.com/</a></p><p>pytorch底层源码解析（一个很不错的博主）</p><p><a href="https://www.cnblogs.com/rossiXYZ/category/1626268.html?page=5">https://www.cnblogs.com/rossiXYZ/category/1626268.html?page=5</a></p><h3 id="手写深度学习入门项目"><a href="#手写深度学习入门项目" class="headerlink" title="手写深度学习入门项目"></a>手写深度学习入门项目</h3><p>小土堆 pytorch学习</p><p><a href="https://space.bilibili.com/203989554?spm_id_from=333.337.search-card.all.click">https://space.bilibili.com/203989554</a></p><p>霹雳吧啦Wz 图像分类篇章 以及目标检测</p><p><a href="https://space.bilibili.com/18161609/channel/collectiondetail?sid=48290">https://space.bilibili.com/18161609/channel/collectiondetail?sid=48290</a></p><p>手写YOLO系列和fast rcnn系列：</p><p><a href="https://www.bilibili.com/video/BV1JR4y1g77H?spm_id_from=333.999.0.0&vd_source=a6509cab8ccb8b81d6a70af693cc008f">https://www.bilibili.com/video/BV1JR4y1g77H</a></p><p><a href="https://space.bilibili.com/472467171">https://space.bilibili.com/472467171</a></p><h3 id="开源库-项目"><a href="#开源库-项目" class="headerlink" title="开源库/项目"></a>开源库/项目</h3><p>OpenMMLab</p><p><a href="https://openmmlab.com/">https://openmmlab.com/</a></p><p><a href="https://github.com/open-mmlab">https://github.com/open-mmlab</a></p><p>paddle</p><p><a href="https://github.com/PaddlePaddle">https://github.com/PaddlePaddle</a></p><p>Deep Learning Paper Implementations</p><p><a href="https://github.com/labmlai/annotated_deep_learning_paper_implementations">https://github.com/labmlai/annotated_deep_learning_paper_implementations</a></p><p>Awesome Machine Learning</p><p><a href="https://github.com/josephmisiti/awesome-machine-learning">https://github.com/josephmisiti/awesome-machine-learning</a></p><p>Awesome Deep Learning</p><p><a href="https://github.com/ChristosChristofidis/awesome-deep-learning">https://github.com/ChristosChristofidis/awesome-deep-learning</a></p><p>【杂谈】GitHub上的机器学习/深度学习综述项目合集 - 言有三的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/60245227">https://zhuanlan.zhihu.com/p/60245227</a></p><p>华校专，曾任阿里巴巴资深算法工程师 多年以来学习总结的笔记（机器学习和深度学习）</p><p><a href="https://www.huaxiaozhuan.com/">https://www.huaxiaozhuan.com/</a></p><p>有关实践方法论的总结：<a href="https://www.huaxiaozhuan.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/chapters/18_practical.html">https://www.huaxiaozhuan.com/深度学习/chapters/18_practical.html</a></p><p>王树森（小红书算法工程师、基础模型团队负责人）老师的推荐系统、强化学习相关课程</p><p><a href="https://github.com/wangshusen/RecommenderSystem">https://github.com/wangshusen/RecommenderSystem</a></p><p><a href="https://github.com/wangshusen/DRL">https://github.com/wangshusen/DRL</a></p><h3 id="在线数据集网站"><a href="#在线数据集网站" class="headerlink" title="在线数据集网站"></a>在线数据集网站</h3><p><a href="https://universe.roboflow.com/">https://universe.roboflow.com/</a></p><p>NLPDataSet（刘聪NLP收集的各种nlp数据集，接近50个。。。）</p><p><a href="https://github.com/liucongg/NLPDataSet">https://github.com/liucongg/NLPDataSet</a></p><h3 id="3D感知相关"><a href="#3D感知相关" class="headerlink" title="3D感知相关"></a>3D感知相关</h3><p>从零开始搭一套激光SLAM出来, 通过代码的角度一点一点地深入学习激光SLAM.</p><p><a href="https://github.com/xiangli0608/Creating-2D-laser-slam-from-scratch">https://github.com/xiangli0608/Creating-2D-laser-slam-from-scratch</a></p><h3 id="NVIDIA环境配置常见问题"><a href="#NVIDIA环境配置常见问题" class="headerlink" title="NVIDIA环境配置常见问题"></a>NVIDIA环境配置常见问题</h3><ul><li>显卡驱动安装快速方法：</li></ul><p>ubuntu-drivers devices</p><p>sudo apt install  输入显示的推荐版本</p><ul><li>CUDA与cuDNN的安装：（直接官网选择）</li></ul><p>注意：a100和3090ti不支持cuda11以下，请装113以上的版本。</p><p>推荐装cuda的时候可以用sh的模式，然后顺带安装了驱动（之前就不用装了）对动态库的默认支持更好</p><p>教程可参考：<a href="https://blog.csdn.net/tangjiahao10/article/details/125227005">https://blog.csdn.net/tangjiahao10/article/details/125227005</a></p><p><a href="https://blog.csdn.net/weixin_37926734/article/details/123033286">https://blog.csdn.net/weixin_37926734/article/details/123033286</a></p><p>cudnn的卸载可参考：<a href="https://zhuanlan.zhihu.com/p/83971195">https://zhuanlan.zhihu.com/p/83971195</a></p><p>(注意，这里默认是最新版本的，你需要在右下角进入档案选择对应版本安装,或在下面的网页中找到archive）</p><ul><li><a href="https://developer.nvidia.com/cuda-toolkit-archive">Archive of Previous CUDA Releases</a></li></ul><p><a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></p><p><a href="https://developer.nvidia.com/rdp/cudnn-archive">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p>cudnn的官方安装教程（通过tar文件自己cp或者根据教程安装deb）</p><p><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html">https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html</a></p><p>TensorRT的安装（参考Debian Installation）</p><p><a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian">https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-debian</a></p><ul><li>安装cuda结束后记得把这两个命令加入到~/.bashrc 然后source。</li></ul><p><strong>如果你想要修改版本，可以修改cuda的软链接（选择其他版本的cuda），或者在bashrc中的cuda后面加上版本，如cuda-11.2</strong></p><p>（具体的安装和卸载也可以参考<a href="https://flywine.blog.csdn.net/article/details/81879514">https://flywine.blog.csdn.net/article/details/81879514</a>）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">&quot;/usr/local/cuda/bin:<span class="variable">$PATH</span>&quot;</span> </span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;/usr/local/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span>&quot;</span> </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>解决nvcc版本不一致问题（可能的方法，更换runtime映射</p><p><a href="https://qiyuan-z.github.io/2022/01/06/%E8%A7%A3%E5%86%B3nvidia-smi%E5%92%8Cnvcc%E6%98%BE%E7%A4%BA%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%89%80%E5%AE%89%E8%A3%85CUDA%E7%89%88%E6%9C%AC%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/">https://qiyuan-z.github.io/2022/01/06/解决nvidia-smi和nvcc显示信息与所安装CUDA版本不一致问题/</a> </p><p>解决cudnn找不到 问题（软连接到系统库）</p><p><a href="https://blog.csdn.net/qq451882471/article/details/106967942">https://blog.csdn.net/qq451882471/article/details/106967942</a></p><p>CUDA GPG Repository Key</p><p><a href="https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772">https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772</a></p><p>Tensorrt X docker环境搭建（现在（20221228）tensorrt-ubuntu已经支持deb安装，如果tar就选linux的）</p><p><a href="https://blog.csdn.net/hxj0323/article/details/115859174">https://blog.csdn.net/hxj0323/article/details/115859174</a></p><p>update后nvidia报GPG相关问题（我在18.04的docker遇到）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /&#x27;</span> &gt; /etc/apt/sources.list.d/cuda.list</span><br><span class="line">apt-key adv --recv-keys --keyserver keyserver.ubuntu.com  $(加入报错的密钥)</span><br><span class="line">apt update</span><br></pre></td></tr></table></figure><ul><li>解决类似 <code>error code is libcuda.so: cannot open shared object file: No such file or directory</code>的问题：<a href="https://stackoverflow.com/questions/54249577/importerror-libcuda-so-1-cannot-open-shared-object-file">https://stackoverflow.com/questions/54249577/importerror-libcuda-so-1-cannot-open-shared-object-file</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此处我们搜索的是libcuda.so</span></span><br><span class="line">sudo find /usr/ -name <span class="string">&#x27;libcuda.so&#x27;</span> <span class="comment">#有时候搜索的是libcuda.so.*</span></span><br><span class="line"><span class="comment"># 这一步是为了找到所在文件夹，我在WSL中搜到了几个文件夹下，我任意加入一个文件夹到LD_PATH：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假如前一步发现libcuda的位置为：/usr/lib/wsl/lib/libcuda.so</span></span><br><span class="line"><span class="comment"># 告诉系统要在这里找，你也可以把这句话加入到~/.bashrc然后source ~/.bashrc</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda/targets/x86_64-linux/lib  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此时再次运行就不会报错了！</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>快速下载torch安装包（wget下载然后直接pip install）</p><p><a href="https://download.pytorch.org/whl/torch/">https://download.pytorch.org/whl/torch/</a></p><p>孪生神经网络的相关实现：</p><p><a href="https://blog.csdn.net/weixin_44791964/article/details/107406072">https://blog.csdn.net/weixin_44791964/article/details/107406072</a></p><p><a href="https://blog.csdn.net/lx_ros/article/details/124439120">https://blog.csdn.net/lx_ros/article/details/124439120</a></p><p>深度学习500问</p><p><a href="https://github.com/shliang0603/Awesome-DeepLearning-500FAQ">https://github.com/shliang0603/Awesome-DeepLearning-500FAQ</a></p><h3 id="MLsystem"><a href="#MLsystem" class="headerlink" title="MLsystem"></a>MLsystem</h3><p>ML system 入坑指南</p><p><a href="https://zhuanlan.zhihu.com/p/608318764">https://zhuanlan.zhihu.com/p/608318764</a></p><p>微软出品 人工智能系统</p><p><a href="https://github.com/microsoft/AI-System">https://github.com/microsoft/AI-System</a></p><p>《机器学习系统：设计和实现》（个人觉得很好<br><a href="https://openmlsys.github.io/#">https://openmlsys.github.io/#</a></p><p>MLIR 文章视频汇总（MLIR目的是做一个通用、可复用的编译器框架</p><p><a href="https://zhuanlan.zhihu.com/p/141256429">https://zhuanlan.zhihu.com/p/141256429</a></p><h3 id="深度学习部署"><a href="#深度学习部署" class="headerlink" title="深度学习部署"></a>深度学习部署</h3><p>GiantPandaCV</p><p>国内最好的部署相关公众平台之一，涉及部署的内容比较多且硬核，五星推荐。</p><p><a href="http://giantpandacv.com/resources/">http://giantpandacv.com/resources/</a></p><p><a href="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/">http://giantpandacv.com/project/部署优化/</a></p><p>ncnn推理框架开发版测试</p><p><a href="https://zhuanlan.zhihu.com/p/458139435">https://zhuanlan.zhihu.com/p/458139435</a></p><p>Build &amp; Share Delightful Machine Learning Apps</p><p><a href="https://gradio.app/">https://gradio.app/</a></p><p>Optimum :</p><p> Optimum is an extension of 🤗 Transformers that provides a set of performance optimization tools to train and run models on targeted hardware with maximum efficiency.</p><p><a href="https://huggingface.co/docs/optimum/index">https://huggingface.co/docs/optimum/index</a></p><h4 id="推理-amp-加速量化框架"><a href="#推理-amp-加速量化框架" class="headerlink" title="推理&amp;加速量化框架"></a>推理&amp;加速量化框架</h4><p>ncnn</p><p><a href="https://github.com/Tencent/ncnn">https://github.com/Tencent/ncnn</a></p><p>ncnn源码阅读学习</p><p><a href="https://blog.csdn.net/sinat_31425585/category_9312419.html">https://blog.csdn.net/sinat_31425585/category_9312419.html</a></p><p>openvino</p><p><a href="https://space.bilibili.com/38566875">https://space.bilibili.com/38566875</a></p><p>bbuf老师的onnx学习笔记</p><p><a href="https://zhuanlan.zhihu.com/p/346511883">ONNX学习笔记 - 知乎 (zhihu.com)</a></p><p>TVM官方中文手册</p><p><a href="https://tvm.hyper.ai/docs/">https://tvm.hyper.ai/docs/</a></p><p>tensorrt 插件自生成（腾讯TPAT）</p><p><a href="https://github.com/Tencent/TPAT">https://github.com/Tencent/TPAT</a></p><p>大缺弦的在线onnx转换器</p><p><a href="https://convertmodel.com/">https://convertmodel.com/</a></p><p>很好的量化工具——PPQ</p><p><a href="https://github.com/openppl-public/ppq">https://github.com/openppl-public/ppq</a></p><h4 id="实例参考"><a href="#实例参考" class="headerlink" title="实例参考"></a>实例参考</h4><p>各种开发版的基础功能调通</p><p><a href="https://blog.csdn.net/sxj731533730">https://blog.csdn.net/sxj731533730</a></p><p>待测试</p><p><a href="https://www.zhihu.com/people/li-li-fu-70/posts">成蹊 - 知乎 (zhihu.com)</a></p><p>paddle_to_openvino算子开发</p><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/5241605?channelType=0&channel=0">https://aistudio.baidu.com/aistudio/projectdetail/5241605?channelType=0&amp;channel=0</a></p><p>高性能推理，TensorRT C++/Python库，tensorrt学习参考</p><p><a href="https://github.com/shouxieai/tensorRT_Pro">https://github.com/shouxieai/tensorRT_Pro</a></p><p>A lite C++ toolkit of awesome AI models with ONNXRuntime, NCNN, MNN and TNN. YOLOv5, YOLOX, YOLOP, YOLOv6, YOLOR, MODNet, YOLOX, YOLOv7, YOLOv8. MNN, NCNN, TNN, ONNXRuntime.</p><p><a href="https://github.com/DefTruth/lite.ai.toolkit/blob/main/README.zh.md">https://github.com/DefTruth/lite.ai.toolkit/blob/main/README.zh.md</a></p><p>onxxruntime源码带读</p><p><a href="https://zhuanlan.zhihu.com/p/530925674">https://zhuanlan.zhihu.com/p/530925674</a></p><p>美团视觉GPU推理服务部署架构优化实践（美团关于GPU的推理部署有很多不错的文章</p><p><a href="https://zhuanlan.zhihu.com/p/605094862">https://zhuanlan.zhihu.com/p/605094862</a></p><p>NCNN底层源码带读</p><p><a href="https://zhuanlan.zhihu.com/p/588809520">https://zhuanlan.zhihu.com/p/588809520</a></p><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><h3 id="anaconda基础"><a href="#anaconda基础" class="headerlink" title="anaconda基础"></a><strong>anaconda基础</strong></h3><ul><li>国内的anaconda镜像下载</li></ul><p><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a></p><ul><li>(windows)anaconda与Jupyter notebook安装教程</li></ul><p><a href="https://zhuanlan.zhihu.com/p/37093476">https://zhuanlan.zhihu.com/p/37093476</a></p><ul><li>(linux)anaconda安装教程<ul><li>下载deb格式的anaconda安装包</li><li>安装后在终端使用source ~/.bashrc即可在终端看到(base)标识（不要在管理员模式下运行）</li></ul></li><li>anaconda 换源(记得更换源的时候删去default 以及备份原来的）</li></ul><p><a href="https://blog.csdn.net/qq_33590958/article/details/103291206">https://blog.csdn.net/qq_33590958/article/details/103291206</a></p><ul><li>pip -i镜像源合集（个人喜欢用百度的）</li></ul><p><a href="https://www.cnblogs.com/sunnydou/p/5801760.html">https://www.cnblogs.com/sunnydou/p/5801760.html</a></p><ul><li>非conda pip直接换源（conf）</li></ul><p><a href="https://www.runoob.com/w3cnote/pip-cn-mirror.html">https://www.runoob.com/w3cnote/pip-cn-mirror.html</a></p><ul><li>requirements.txt的生成教程</li></ul><p><a href="https://www.cnblogs.com/lvjinfeng/articles/16333180.html">https://www.cnblogs.com/lvjinfeng/articles/16333180.html</a></p><ul><li>conda与pip虚拟环境导出与转移（方便移植）</li></ul><p><a href="https://blog.csdn.net/weixin_42272869/article/details/122471357">https://blog.csdn.net/weixin_42272869/article/details/122471357</a></p><ul><li>conda常见命令和疑难问题解答：</li></ul><p>有时候你可能会遇到类似<code>Solving environment: failed with initial frozen solve. Retrying with flexible solve.</code>的问题，先耐心等等！让他遍历重试完各个库（我在conda安装cling的时候遇到）</p><p>如果还是有问题再按照网上的方法进行更新conda或者重新安装conda。（或者不要在base下安装）</p><p>删除虚拟环境：<code>conda remove -n ENV_NAME —all</code></p><p>conda更新：<code>conda update anaconda</code></p><p>conda所有库更新：<code>conda update --all</code></p><ul><li>导出自己安装的那些包（freeze是全部！）：使用pipreqs库</li><li>powershell下看不到（base）之类的虚拟库信息，显示出脚本安全问题无法启用，可在powershell管理员模式下输入<code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned</code>即可解决问题。</li></ul><p>awesome项目（包含了绝大部分的python相关资源）</p><p><a href="https://github.com/vinta/awesome-python">https://github.com/vinta/awesome-python</a></p><p><a href="http://jobbole.github.io/awesome-python-cn/">http://jobbole.github.io/awesome-python-cn/</a></p><p>Python Cookbook 3rd Edition</p><p><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/index.html">https://python3-cookbook.readthedocs.io/zh_CN/latest/index.html</a></p><p>Python并行编程</p><p><a href="https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html">https://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html</a></p><p>Python 3 标准库实例教程(真正现代、进阶的python教程</p><p><a href="https://learnku.com/docs/pymotw">https://learnku.com/docs/pymotw</a></p><p>pandas教程</p><p><a href="https://pandas.pydata.org/docs/getting_started/install.html">https://pandas.pydata.org/docs/getting_started/install.html</a></p><p>或者可以看看datawhale的教程</p><p>Scipy Lecture Notes//Advanced Python Constructs//Advanced NumP</p><p><a href="http://scipy-lectures.org/index.html">http://scipy-lectures.org/index.html</a></p><p>SICP Python 描述 中文版</p><p><a href="https://wizardforcel.gitbooks.io/sicp-py/content/">https://wizardforcel.gitbooks.io/sicp-py/content/</a></p><p>opencv图像处理100问，可用来查缺补漏（有些格式问题但不影响</p><p><a href="https://github.com/gzr2017/ImageProcessing100Wen">https://github.com/gzr2017/ImageProcessing100Wen</a></p><p>有趣的Python爬虫和Python数据分析小项目（有些方法可能因为是3年前的东西会失效</p><p><a href="https://github.com/Alfred1984/interesting-python">https://github.com/Alfred1984/interesting-python</a></p><p>requests库官方手册（交互常用，主要需理解请求头等）</p><p><a href="https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request">https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request</a></p><h3 id="实例项目等"><a href="#实例项目等" class="headerlink" title="实例项目等"></a>实例项目等</h3><p>supervisor + gunicorn + flask 高并发的接口 + 完整（标准）的日志部署</p><p><a href="https://zhuanlan.zhihu.com/p/79227989">https://zhuanlan.zhihu.com/p/79227989</a></p><p>教你阅读 Python 开源项目代码（里面有一些基础开源项目可以参考）</p><p><a href="https://learnku.com/articles/23010/teach-you-to-read-the-python-open-source-project-code">https://learnku.com/articles/23010/teach-you-to-read-the-python-open-source-project-code</a></p><p>很不错的python状态机（可以画成图）展示工具：</p><p><a href="https://github.com/laike9m/Cyberbrain">https://github.com/laike9m/Cyberbrain</a></p><h2 id="C与汇编"><a href="#C与汇编" class="headerlink" title="C与汇编"></a>C与汇编</h2><p>翁恺的相关视频(入门和进阶)</p><p><a href="https://www.icourse163.org/u/wengkai?userId=318013">https://www.icourse163.org/u/wengkai?userId=318013</a></p><p>100个GDB小技巧：</p><p><a href="https://wizardforcel.gitbooks.io/100-gdb-tips/content/part1.html">https://wizardforcel.gitbooks.io/100-gdb-tips/content/part1.html</a></p><p>标准库收录网站</p><p><a href="https://www.cplusplus.com/reference/">https://www.cplusplus.com/reference/</a></p><p>汇编语言在线解析网站</p><p><a href="https://godbolt.org/">https://godbolt.org/</a></p><p>内联汇编学习</p><p><a href="https://baijiahao.baidu.com/s?id=1722268508697136684">https://baijiahao.baidu.com/s?id=1722268508697136684</a></p><p><a href="https://www.jianshu.com/p/1782e14a0766">https://www.jianshu.com/p/1782e14a0766</a></p><p>“undefined reference to XXX”问题总结</p><p><a href="https://github.com/Captain1986/CaptainBlackboard/blob/master/D#0001-undefined_reference_to_XXX/D#0001.md">https://github.com/Captain1986/CaptainBlackboard/blob/master/D%230001-undefined_reference_to_XXX/D%230001.md</a></p><p>有关硬件开发（嵌入式）的推荐个人博客列表</p><p><a href="https://github.com/JesseGuoX/DoHard">https://github.com/JesseGuoX/DoHard</a></p><p>A curated list of C good stuff. </p><p>This project does <em>not</em> index anything C++-related; only pure C stuff is considered.</p><p><a href="https://github.com/sanbuphy/awesome-c">https://github.com/sanbuphy/awesome-c</a></p><p>LLVM编译过程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/llvm/llvm-project/releases/download/llvmorg-10.0.0/llvm-10.0.0.src.tar.xz</span><br><span class="line">tar xvJf llvm-10.0.0.src.tar.xz</span><br><span class="line"><span class="built_in">cd</span> llvm-10.0.0.src</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake .. -DLLVM_ENABLE_RTTI:BOOL=ON -DBUILD_SHARED_LIBS:BOOL=OFF -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=“X86;NVPTX” -DLLVM_ENABLE_ASSERTIONS=ON</span><br><span class="line"><span class="comment"># 如果你想在 NVIDIA Jetson TX2 上进行构建, 请使用 -DLLVM_TARGETS_TO_BUILD=&quot;ARM;NVPTX&quot;</span></span><br><span class="line">make -j 8</span><br><span class="line">sudo make install</span><br><span class="line"><span class="comment"># 检查你安装的 LLVM 版本</span></span><br><span class="line">llvm-config —version  <span class="comment"># 应该是 10.0.0</span></span><br></pre></td></tr></table></figure><p>C语言的jupyter notebook拓展安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter-c-kernel</span><br><span class="line">install_c_kernel</span><br><span class="line">jupyter kernelspec list</span><br></pre></td></tr></table></figure><h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><p><strong>c++入门学习（看自己兴趣按需索取）</strong></p><ul><li>浙大翁恺（简单，适合快速过一遍）：<a href="https://www.bilibili.com/video/BV1dE41167hJ?p=34">https://www.bilibili.com/video/BV1dE41167hJ?p=34</a></li><li>南科大于仕琪（现代化，十分推荐）<a href="https://www.bilibili.com/video/BV1Vf4y1P7pq?p=1">https://www.bilibili.com/video/BV1Vf4y1P7pq?p=1</a></li><li>侯捷老师视频（看完于老师可以无缝衔接，资源就不公开了，推荐看完1、2、3）</li></ul><p>c++的jupyter notebook 扩展（用来写小作业）</p><p><code>conda install -c conda-forge xeus-cling</code></p><p><code>conda install -c conda-forge jupyterlab</code></p><p>然后输入 jupyter lab 就可以看到有c11到17内核支持的了。</p><p>如果想要在vscode使用，只要复制带token的那一大串东西然后在vscode打开jupyter下面找到server连接远程即可。</p><p>c++ 在线测试：</p><p><a href="https://cpp.sh/">https://cpp.sh/</a></p><p>各类语言在线解析网站，包括汇编调试和各种C++标准下的insight函数</p><p><a href="https://godbolt.org/">https://godbolt.org/</a></p><p>awesome项目（包含了绝大部分的c++相关资源）</p><p><a href="https://github.com/fffaraz/awesome-cpp">https://github.com/fffaraz/awesome-cpp</a></p><p><a href="http://jobbole.github.io/awesome-python-cn/">http://jobbole.github.io/awesome-python-cn/</a></p><p>cmake 快速编译安装：</p><p>前往官网下载最新发行版：<a href="https://cmake.org/download/">https://cmake.org/download/</a>     然后如下运行即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卸载旧版本</span></span><br><span class="line">sudo apt-get autoremove cmake</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装新版本</span></span><br><span class="line">./configure</span><br><span class="line">make -j20</span><br><span class="line">sudo make install</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果此时cmake --verion 没有变化或者不存在，把下面这个加入程序到环境变量即可</span></span><br><span class="line">vim ~/.bashrc</span><br><span class="line"><span class="comment"># export PATH=/usr/local/share/cmake-3.25:$PATH</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc   </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Modern CMake 简体中文版</p><p><a href="https://modern-cmake-cn.github.io/Modern-CMake-zh_CN/">https://modern-cmake-cn.github.io/Modern-CMake-zh_CN/</a></p><p>cmakelist生成的makefile调试用make VERBOSE=1 而不是 make -nb</p><p>cmake快速入门</p><p><a href="https://juejin.cn/post/6844903557183832078">https://juejin.cn/post/6844903557183832078</a></p><p>CMake菜谱（CMake Cookbook中文版）（面向实际应用小工具，推荐）</p><p><a href="https://www.bookstack.cn/read/CMake-Cookbook/README.md">https://www.bookstack.cn/read/CMake-Cookbook/README.md</a></p><p>C++ reference（字典）</p><p><a href="https://en.cppreference.com/w/">https://en.cppreference.com/w/</a></p><p>c++并发编程</p><p><a href="https://paul.pub/cpp-concurrency/">https://paul.pub/cpp-concurrency/</a></p><p>双笙子佯谬    图形学大佬，Zeno和Taichi Blend的作者</p><p><a href="https://space.bilibili.com/263032155">https://space.bilibili.com/263032155</a></p><p>C++ Core Guidelines </p><p><a href="https://github.com/isocpp/CppCoreGuidelines">https://github.com/isocpp/CppCoreGuidelines</a></p><p>Google C++ Style Guide</p><p><a href="https://google.github.io/styleguide/cppguide.html">https://google.github.io/styleguide/cppguide.html</a></p><p>c++手写数据库练习 CMU 15-445: Database Systems</p><p><a href="https://csdiy.wiki/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/15445/">https://csdiy.wiki/数据库系统/15445/</a></p><p>C++ Standard Draft Sources（一起成为语言律师）</p><p><a href="https://github.com/cplusplus/draft">https://github.com/cplusplus/draft</a></p><p>详细的C/C++编程规范指南，由360质量工程部编著，适用于桌面、服务端及嵌入式软件系统。</p><p><a href="https://github.com/Qihoo360/safe-rules">https://github.com/Qihoo360/safe-rules</a></p><p>程序可移植性保证cmake。可获取系统信息、编译器、平台、指令集等信息。</p><p><a href="https://www.bookstack.cn/read/CMake-Cookbook/content-chapter2-2.5-chinese.md">https://www.bookstack.cn/read/CMake-Cookbook/content-chapter2-2.5-chinese.md</a></p><h3 id="C-的杂物间"><a href="#C-的杂物间" class="headerlink" title="C++的杂物间"></a>C++的杂物间</h3><p>DJI thermal analysis tool  相关教程（日文</p><p><a href="https://qiita.com/tutu/items/b5cf2b39dd30786d9064">https://qiita.com/tutu/items/b5cf2b39dd30786d9064</a></p><h3 id="音视频相关"><a href="#音视频相关" class="headerlink" title="音视频相关"></a>音视频相关</h3><p>音视频原理必看国内大神-雷神</p><p><a href="https://blog.csdn.net/leixiaohua1020/article/details/18893769">https://blog.csdn.net/leixiaohua1020/article/details/18893769</a></p><p>ffmpeg原理 罗上文</p><p><a href="https://ffmpeg.xianwaizhiyin.net/cover.html">https://ffmpeg.xianwaizhiyin.net/cover.html</a></p><h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><p>适合入门的小型Python编译器项目（包含cuda相关内容）</p><p><a href="https://zhuanlan.zhihu.com/p/603352525">https://zhuanlan.zhihu.com/p/603352525</a></p><p>cuda学习教程</p><p><a href="https://space.bilibili.com/37270391/channel/seriesdetail?sid=1454805">https://space.bilibili.com/37270391/channel/seriesdetail?sid=1454805</a></p><p>CUDA-Programming-Guide-in-Chinese</p><p><a href="https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese">https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese</a></p><p>CUDA C++ Programming Guide</p><p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</a></p><h2 id="程序性能优化"><a href="#程序性能优化" class="headerlink" title="程序性能优化"></a>程序性能优化</h2><p>性能优化实战收集（包括听风扇声音测性能）</p><p><a href="https://github.com/plantegg/programmer_case">https://github.com/plantegg/programmer_case</a></p><h2 id="学术论文"><a href="#学术论文" class="headerlink" title="学术论文"></a>学术论文</h2><p>查询接受率的网站:</p><p><a href="https://www.openresearch.org/wiki/Main_Page">https://www.openresearch.org/wiki/Main_Page</a></p><p>LaTeX 图片转代码</p><p><a href="https://mathf.itewqq.cn/">点这里mathF</a></p><p><a href="https://web.baimiaoapp.com/image-to-latex">https://web.baimiaoapp.com/image-to-latex</a></p><p>LaTeX手写字符识别（不知道字符的代码是什么的时候）</p><p><a href="http://detexify.kirelabs.org/classify.html">http://detexify.kirelabs.org/classify.html</a></p><p>LaTeX开源OCR方案</p><p><a href="https://github.com/lukas-blecher/LaTeX-OCR">https://github.com/lukas-blecher/LaTeX-OCR</a></p><p>论文翻译</p><p><a href="https://tongtianta.site/">https://tongtianta.site/</a></p><p>一文网尽CV/Robotics顶会论文常用高级词汇/句式！ by 叶小飞（推荐关注）</p><p><a href="https://zhuanlan.zhihu.com/p/415926905">https://zhuanlan.zhihu.com/p/415926905</a></p><p>AI论文检索</p><p><a href="https://elicit.org/">https://elicit.org/</a></p><h2 id="其他日常使用网站"><a href="#其他日常使用网站" class="headerlink" title="其他日常使用网站"></a>其他日常使用网站</h2><p>有关思维导图的代码（类似markdown）生成：</p><p><a href="https://xzmind.xuanzi.ltd/apps.html">https://xzmind.xuanzi.ltd/apps.html</a></p><p>流程图绘制：</p><p><a href="https://app.diagrams.net/">https://app.diagrams.net/</a></p><p>json可视化：</p><p><a href="https://c.runoob.com/front-end/53/">https://c.runoob.com/front-end/53/</a></p><p>快速文件传输（随意分享给人不用网盘）(拷贝兔也可以）</p><p><a href="https://www.wenshushu.cn/">https://www.wenshushu.cn/</a></p><p>偏极客的新闻网，无广告，而且有一套防刷热度算法，也不搞推荐算法</p><p><a href="https://news.ycombinator.com/news">https://news.ycombinator.com/news</a></p><p>快速拼接图像和其他常用图片处理工具</p><p><a href="http://www.atoolbox.net/Tool.php?Id=978">http://www.atoolbox.net/Tool.php?Id=978</a></p><p>黑白照片上色</p><p><a href="https://palette.fm/">https://palette.fm/</a></p><p>让你“爱”上 GitHub，解决访问时图裂、加载慢的问题。（无需安装）</p><p><a href="https://github.com/521xueweihan/GitHub520">https://github.com/521xueweihan/GitHub520</a></p><h2 id="有趣的项目"><a href="#有趣的项目" class="headerlink" title="有趣的项目"></a>有趣的项目</h2><p>比disco diffusion更强大的绘制工具SD：</p><p>在自己电脑运行Stable Diffusion和完整项目下载</p><p><a href="https://mp.weixin.qq.com/s/syEkqbBSmTwdi_cPB6Kd3g">https://mp.weixin.qq.com/s/syEkqbBSmTwdi_cPB6Kd3g</a></p><p>StableDiffusion Int8量化教程与ONNX导出推理</p><p><a href="https://mp.weixin.qq.com/s/18EIga7w9y1FG0oWcnysIw">https://mp.weixin.qq.com/s/18EIga7w9y1FG0oWcnysIw</a></p><p>ChatGPT 中文调教指南</p><p><a href="https://github.com/PlexPt/awesome-chatgpt-prompts-zh">https://github.com/PlexPt/awesome-chatgpt-prompts-zh</a></p><p>CodeGeeX: 多语言代码生成模型（代码生成与代码翻译）</p><p><a href="https://github.com/THUDM/CodeGeeX/blob/main/README_zh.md">https://github.com/THUDM/CodeGeeX/blob/main/README_zh.md</a></p><p>Whisper AI剪视频小工具</p><p><a href="https://www.bilibili.com/video/BV1Pe4y1t7de/">https://www.bilibili.com/video/BV1Pe4y1t7de/</a></p><p><a href="https://github.com/mli/autocut/">https://github.com/mli/autocut/</a></p><h2 id="有趣的故事"><a href="#有趣的故事" class="headerlink" title="有趣的故事"></a>有趣的故事</h2><p>谷歌背后的数学</p><p><a href="https://www.changhai.org/articles/technology/misc/google_math.php">https://www.changhai.org/articles/technology/misc/google_math.php</a></p><p>火光摇曳(数学科普) Rickjin(靳志辉)   （非常好传递了统计之美）</p><p><a href="https://uploads.cosx.org/2014/07/gamma.pdf">https://uploads.cosx.org/2014/07/gamma.pdf</a></p><p>计算的极限</p><p><a href="https://fwjmath.wordpress.com/recommended-list/">https://fwjmath.wordpress.com/recommended-list/</a></p><h2 id="心理健康建设"><a href="#心理健康建设" class="headerlink" title="心理健康建设"></a>心理健康建设</h2><p>如何在工作中学习（好的方法论）</p><p><a href="https://plantegg.github.io/2018/05/24/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0V1.1/">https://plantegg.github.io/2018/05/24/如何在工作中学习V1.1/</a></p><p>2017年买房经历总结出来的买房购房知识</p><p><a href="https://github.com/houshanren/hangzhou_house_knowledge">https://github.com/houshanren/hangzhou_house_knowledge</a></p><p>2022年杭州购房指南</p><p><a href="http://github.com/zkqiang/hangzhou-house-guide">github.com/zkqiang/hangzhou-house-guide</a> </p><p>2020年11月上海购房指南</p><p><a href="http://github.com/ayuer/shanghai_house_knowledge">github.com/ayuer/shanghai_house_knowledge</a></p><h2 id="英语"><a href="#英语" class="headerlink" title="英语"></a>英语</h2><h3 id="英文语法在线修改"><a href="#英文语法在线修改" class="headerlink" title="英文语法在线修改"></a>英文语法在线修改</h3><p><a href="https://www.grammarly.com/">https://www.grammarly.com/</a></p><p><a href="https://www.nounplus.net/grammarcheck/">https://www.nounplus.net/grammarcheck/</a></p><p><a href="https://virtualwritingtutor.com/">https://virtualwritingtutor.com/</a></p><h3 id="英文论文好用工具"><a href="#英文论文好用工具" class="headerlink" title="英文论文好用工具"></a>英文论文好用工具</h3><p>TextRanch 句子参考</p><p><a href="https://textranch.com/">https://textranch.com/</a></p><p>QuillBot 文段改写</p><p><a href="https://quillbot.com/">https://quillbot.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> 计算机科学 </category>
          
          <category> 学习资料 </category>
          
          <category> 学习指南 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/02/26/hello-world/"/>
      <url>/2023/02/26/hello-world/</url>
      
        <content type="html"><![CDATA[<p>这是一个测试</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
